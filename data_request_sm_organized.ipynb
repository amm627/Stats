{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Moisture Data Sources\n",
    "\n",
    "This notebook implements standardized retrievers for multiple soil moisture data sources. Each source has unique characteristics:\n",
    "\n",
    "## Data Source Characteristics\n",
    "\n",
    "### Data Assimilation \n",
    "\n",
    "### ERA5\n",
    "- **Provider**: ECMWF (European Centre for Medium-Range Weather Forecasts)\n",
    "- **Resolution**: 0.1° x 0.1° (approximately 9km)\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Monthly updates, with 2-3 month delay\n",
    "- **Key Features**: High spatial resolution, consistent reanalysis\n",
    "\n",
    "### GLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.25° x 0.25°\n",
    "- **Temporal Coverage**: 2000-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Global coverage, multiple soil layers\n",
    "\n",
    "### NLDAS\n",
    "- **Provider**: NASA/NOAA\n",
    "- **Resolution**: 0.125° x 0.125°\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Hourly\n",
    "- **Key Features**: North American focus, high temporal resolution\n",
    "\n",
    "### FLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.1° x 0.1°\n",
    "- **Temporal Coverage**: 1982-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Africa-focused land data assimilation\n",
    "\n",
    "### MERRA-2\n",
    "- **Provider**: NASA GMAO\n",
    "- **Resolution**: 0.5° x 0.625°\n",
    "- **Temporal Coverage**: 1980-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Comprehensive atmospheric reanalysis\n",
    "\n",
    "### Remote Sensing \n",
    "\n",
    "### SMAP\n",
    "- **Provider**: NASA\n",
    "- **Resolution**: 9km x 9km\n",
    "- **Temporal Coverage**: 2015-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Direct satellite observations, high accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/climate_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import necessary packages \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cdsapi\n",
    "import netCDF4\n",
    "import earthaccess\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import requests\n",
    "from time import sleep\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "from typing import Tuple, Optional,List, Dict\n",
    "from pathlib import Path\n",
    "import ftplib\n",
    "import ssl\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "max_lat= 45.02\n",
    "min_lat= 40.5\n",
    "max_lon= -71.85\n",
    "min_lon= -79.77\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "start_M=\"2013-01\"\n",
    "end_M = \"2023-12\"\n",
    "lat_bounds = (min_lat, max_lat)\n",
    "lon_bounds = (min_lon, max_lon)\n",
    "area=(max_lat, min_lon, min_lat, max_lon) # (max_lat, min_lon, min_lat, max_lon)\n",
    "bbox=(min_lon, min_lat, max_lon, max_lat) # (min_lon, min_lat, max_lon, max_lat)\n",
    "date_range=(start_date, end_date) #start_dat ,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate statistics for the datasets \n",
    "def calculate_statistics(data, var_name, var_attrs):\n",
    "    \"\"\"\n",
    "    Calculate statistics for a variable based on its data type\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        The data array to analyze\n",
    "    var_name : str\n",
    "        Name of the variable\n",
    "    var_attrs : dict\n",
    "        Variable attributes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the statistics\n",
    "    \"\"\"\n",
    "    # Check if data is datetime type\n",
    "    if np.issubdtype(data.dtype, np.datetime64):\n",
    "        return {\n",
    "            'type': 'datetime',\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    # For numeric data\n",
    "    valid_data = data[~np.isnan(data)]\n",
    "    if len(valid_data) > 0:\n",
    "        return {\n",
    "            'type': 'numeric',\n",
    "            'mean': np.nanmean(data),\n",
    "            'median': np.nanmedian(data),\n",
    "            'std': np.nanstd(data),\n",
    "            'var': np.nanvar(data),\n",
    "            'min': np.nanmin(data),\n",
    "            'max': np.nanmax(data),\n",
    "            'valid_points': len(valid_data),\n",
    "            'missing_points': np.sum(np.isnan(data)),\n",
    "            'coverage': (len(valid_data) / data.size * 100),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'type': 'empty',\n",
    "        'shape': data.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era5_monthly_data(\n",
    "    north: float,\n",
    "    south: float,\n",
    "    west: float,\n",
    "    east: float,\n",
    "    output_dir: str = \"data/era5\",\n",
    "    start_year: int = 2013,\n",
    "    end_year: int = 2023,\n",
    "    retry_attempts: int = 3,\n",
    "    retry_delay: int = 60\n",
    ") -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Retrieve ERA5 monthly soil moisture data, combining datasets incrementally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nRetrieving ERA5 monthly data for {start_year}-{end_year}\")\n",
    "        print(f\"Spatial bounds: {north}°N to {south}°N, {west}°E to {east}°E\")\n",
    "        \n",
    "        # Initialize CDS client\n",
    "        client = cdsapi.Client()\n",
    "        \n",
    "        # Define soil moisture variables\n",
    "        variables = [\n",
    "            'volumetric_soil_water_layer_1',  # 0-7 cm\n",
    "            'volumetric_soil_water_layer_2',  # 7-28 cm\n",
    "            'volumetric_soil_water_layer_3',  # 28-100 cm\n",
    "            'volumetric_soil_water_layer_4'   # 100-289 cm\n",
    "        ]\n",
    "        \n",
    "        combined_ds = None\n",
    "        total_months = (end_year - start_year + 1) * 12\n",
    "        \n",
    "        with tqdm(total=total_months, desc=\"Processing months\") as pbar:\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                for month in range(1, 13):\n",
    "                    try:\n",
    "                        output_file = os.path.join(\n",
    "                            output_dir, \n",
    "                            f\"era5_soil_moisture_{year}_{month:02d}.nc\"\n",
    "                        )\n",
    "                        \n",
    "                        if os.path.exists(output_file):\n",
    "                            print(f\"\\nLoading existing file for {year}-{month:02d}\")\n",
    "                            ds_month = xr.open_dataset(output_file)\n",
    "                        else:\n",
    "                            print(f\"\\nRetrieving data for {year}-{month:02d}\")\n",
    "                            \n",
    "                            for attempt in range(retry_attempts):\n",
    "                                try:\n",
    "                                    result = client.retrieve(\n",
    "                                        'reanalysis-era5-single-levels-monthly-means',\n",
    "                                        {\n",
    "                                            'product_type': 'monthly_averaged_reanalysis',\n",
    "                                            'variable': variables,\n",
    "                                            'year': str(year),\n",
    "                                            'month': f\"{month:02d}\",\n",
    "                                            'time': ['00:00'],\n",
    "                                            'area': [north, west, south, east],\n",
    "                                            'format': 'netcdf'\n",
    "                                        }\n",
    "                                    )\n",
    "                                    \n",
    "                                    result.download(output_file)\n",
    "                                    print(f\"Successfully downloaded data for {year}-{month:02d}\")\n",
    "                                    \n",
    "                                    ds_month = xr.open_dataset(output_file)\n",
    "                                    break\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    if attempt < retry_attempts - 1:\n",
    "                                        print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                                        print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                                        time.sleep(retry_delay)\n",
    "                                    else:\n",
    "                                        raise Exception(f\"Failed all {retry_attempts} attempts\")\n",
    "                        \n",
    "                        # Add time encoding if missing\n",
    "                        if 'time' in ds_month.coords:\n",
    "                            ds_month.time.encoding['units'] = 'hours since 1900-01-01'\n",
    "                        \n",
    "                        # Combine with existing dataset\n",
    "                        if combined_ds is None:\n",
    "                            combined_ds = ds_month\n",
    "                        else:\n",
    "                            combined_ds = xr.concat([combined_ds, ds_month], dim='time')\n",
    "                            combined_ds = combined_ds.sortby('time')\n",
    "                        \n",
    "                        # Close monthly dataset to free memory\n",
    "                        ds_month.close()\n",
    "                        \n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {year}-{month:02d}: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        if combined_ds is None:\n",
    "            raise ValueError(\"No data was successfully retrieved\")\n",
    "        \n",
    "        # Add metadata\n",
    "        combined_ds.attrs.update({\n",
    "            'title': 'ERA5 Monthly Soil Moisture Data',\n",
    "            'source': 'Copernicus Climate Data Store',\n",
    "            'time_coverage_start': f\"{start_year}-01\",\n",
    "            'time_coverage_end': f\"{end_year}-12\",\n",
    "            'geospatial_lat_min': south,\n",
    "            'geospatial_lat_max': north,\n",
    "            'geospatial_lon_min': west,\n",
    "            'geospatial_lon_max': east,\n",
    "            'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "        \n",
    "        # Save combined dataset\n",
    "        combined_output_file = os.path.join(\n",
    "            output_dir, \n",
    "            f\"era5_soil_moisture_{start_year}_{end_year}_combined.nc\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSaving combined dataset to {combined_output_file}\")\n",
    "        encoding = {var: {'zlib': True, 'complevel': 5} for var in combined_ds.data_vars}\n",
    "        combined_ds.to_netcdf(combined_output_file, encoding=encoding)\n",
    "        \n",
    "        # [Rest of the function remains the same...]\n",
    "        \n",
    "        return combined_ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving ERA5 monthly data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_era5_monthly_data(\n",
    "    north: float,\n",
    "    south: float,\n",
    "    west: float,\n",
    "    east: float,\n",
    "    output_dir: str = \"data/era5\",\n",
    "    start_year: int = 2013,\n",
    "    end_year: int = 2023,\n",
    "    retry_attempts: int = 3,\n",
    "    retry_delay: int = 60\n",
    ") -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Retrieve ERA5 monthly soil moisture data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    north : float\n",
    "        Northern latitude boundary\n",
    "    south : float\n",
    "        Southern latitude boundary\n",
    "    west : float\n",
    "        Western longitude boundary\n",
    "    east : float\n",
    "        Eastern longitude boundary\n",
    "    output_dir : str\n",
    "        Directory to save output files\n",
    "    start_year : int\n",
    "        Start year (default: 2013)\n",
    "    end_year : int\n",
    "        End year (default: 2023)\n",
    "    retry_attempts : int\n",
    "        Number of retry attempts for failed requests\n",
    "    retry_delay : int\n",
    "        Delay in seconds between retries\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset or None\n",
    "        Combined dataset with ERA5 monthly data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nRetrieving ERA5 monthly data for {start_year}-{end_year}\")\n",
    "        print(f\"Spatial bounds: {north}°N to {south}°N, {west}°E to {east}°E\")\n",
    "        \n",
    "        # Initialize CDS client\n",
    "        client = cdsapi.Client()\n",
    "        \n",
    "        # Define soil moisture variables\n",
    "        variables = [\n",
    "            'volumetric_soil_water_layer_1',  # 0-7 cm\n",
    "            'volumetric_soil_water_layer_2',  # 7-28 cm\n",
    "            'volumetric_soil_water_layer_3',  # 28-100 cm\n",
    "            'volumetric_soil_water_layer_4'   # 100-289 cm\n",
    "        ]\n",
    "        \n",
    "        monthly_datasets = []\n",
    "        total_months = (end_year - start_year + 1) * 12\n",
    "        \n",
    "        with tqdm(total=total_months, desc=\"Processing months\") as pbar:\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                for month in range(1, 13):\n",
    "                    try:\n",
    "                        output_file = os.path.join(\n",
    "                            output_dir, \n",
    "                            f\"era5_soil_moisture_{year}_{month:02d}.nc\"\n",
    "                        )\n",
    "                        \n",
    "                        if os.path.exists(output_file):\n",
    "                            print(f\"\\nLoading existing file for {year}-{month:02d}\")\n",
    "                            ds_month = xr.open_dataset(output_file)\n",
    "                        else:\n",
    "                            print(f\"\\nRetrieving data for {year}-{month:02d}\")\n",
    "                            \n",
    "                            for attempt in range(retry_attempts):\n",
    "                                try:\n",
    "                                    result = client.retrieve(\n",
    "                                        'reanalysis-era5-single-levels-monthly-means',\n",
    "                                        {\n",
    "                                            'product_type': 'monthly_averaged_reanalysis',\n",
    "                                            'variable': variables,\n",
    "                                            'year': str(year),\n",
    "                                            'month': f\"{month:02d}\",\n",
    "                                            'time': ['00:00'],\n",
    "                                            'area': [north, west, south, east],\n",
    "                                            'format': 'netcdf'\n",
    "                                        }\n",
    "                                    )\n",
    "                                    \n",
    "                                    # Download the file\n",
    "                                    result.download(output_file)\n",
    "                                    print(f\"Successfully downloaded data for {year}-{month:02d}\")\n",
    "                                    \n",
    "                                    # Load the data\n",
    "                                    ds_month = xr.open_dataset(output_file)\n",
    "                                    break\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    if attempt < retry_attempts - 1:\n",
    "                                        print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                                        print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                                        time.sleep(retry_delay)\n",
    "                                    else:\n",
    "                                        raise Exception(f\"Failed all {retry_attempts} attempts\")\n",
    "                        \n",
    "                        # Add time encoding if missing\n",
    "                        if 'time' in ds_month.coords:\n",
    "                            ds_month.time.encoding['units'] = 'hours since 1900-01-01'\n",
    "                        \n",
    "                        monthly_datasets.append(ds_month)\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {year}-{month:02d}: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        if not monthly_datasets:\n",
    "            raise ValueError(\"No data was successfully retrieved\")\n",
    "        \n",
    "        # Combine all monthly datasets\n",
    "        print(\"\\nCombining monthly datasets...\")\n",
    "        ds_combined = xr.concat(monthly_datasets, dim='time')\n",
    "        ds_combined = ds_combined.sortby('time')\n",
    "        \n",
    "        # Add metadata\n",
    "        ds_combined.attrs.update({\n",
    "            'title': 'ERA5 Monthly Soil Moisture Data',\n",
    "            'source': 'Copernicus Climate Data Store',\n",
    "            'time_coverage_start': f\"{start_year}-01\",\n",
    "            'time_coverage_end': f\"{end_year}-12\",\n",
    "            'geospatial_lat_min': south,\n",
    "            'geospatial_lat_max': north,\n",
    "            'geospatial_lon_min': west,\n",
    "            'geospatial_lon_max': east,\n",
    "            'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "        \n",
    "        # Save combined dataset\n",
    "        combined_output_file = os.path.join(\n",
    "            output_dir, \n",
    "            f\"era5_soil_moisture_{start_year}_{end_year}_combined.nc\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSaving combined dataset to {combined_output_file}\")\n",
    "        encoding = {var: {'zlib': True, 'complevel': 5} for var in ds_combined.data_vars}\n",
    "        ds_combined.to_netcdf(combined_output_file, encoding=encoding)\n",
    "        \n",
    "        # Print dataset information\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Time range: {ds_combined.time.values[0]} to {ds_combined.time.values[-1]}\")\n",
    "        print(f\"Number of timesteps: {len(ds_combined.time)}\")\n",
    "        if 'latitude' in ds_combined.dims and 'longitude' in ds_combined.dims:\n",
    "            print(f\"Spatial coverage: {ds_combined.sizes['latitude']}x{ds_combined.sizes['longitude']} grid points\")\n",
    "            print(f\"Lat range: {float(ds_combined.latitude.min().values):.3f} to {float(ds_combined.latitude.max().values):.3f}\")\n",
    "            print(f\"Lon range: {float(ds_combined.longitude.min().values):.3f} to {float(ds_combined.longitude.max().values):.3f}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(\"\\nVariable Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        var_descriptions = {\n",
    "            'volumetric_soil_water_layer_1': 'Soil moisture (0-7 cm)',\n",
    "            'volumetric_soil_water_layer_2': 'Soil moisture (7-28 cm)',\n",
    "            'volumetric_soil_water_layer_3': 'Soil moisture (28-100 cm)',\n",
    "            'volumetric_soil_water_layer_4': 'Soil moisture (100-289 cm)'\n",
    "        }\n",
    "        \n",
    "        for var in ds_combined.data_vars:\n",
    "            print(f\"\\nVariable: {var}\")\n",
    "            print(f\"Description: {var_descriptions.get(var, var)}\")\n",
    "            \n",
    "            data = ds_combined[var].values\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "            \n",
    "            if len(valid_data) > 0:\n",
    "                percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                print(\"Statistics:\")\n",
    "                print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                print(f\"  Min:      {percentiles[0]:.4f}\")\n",
    "                print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                print(f\"  Max:      {percentiles[4]:.4f}\")\n",
    "                print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "        \n",
    "        return ds_combined\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving ERA5 monthly data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLDAS data request \n",
    "def get_nldas_data(start_date, end_date, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Retrieve NLDAS soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with NLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving NLDAS soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        # Authenticate with NASA Earthdata\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        # Search for granules\n",
    "        print(\"\\nSearching for NLDAS granules...\")\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"NLDAS_NOAH0125_M\",\n",
    "            version=\"2.0\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No NLDAS granules found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            \n",
    "            # Use tqdm for processing progress\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Select only soil moisture variables (SoilM_*)\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilM_' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting\n",
    "                    if 'lat' in ds.dims:\n",
    "                        ds = ds.sel(lat=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    if 'lon' in ds.dims:\n",
    "                        ds = ds.sel(lon=slice(lon_bounds[0], lon_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            # Print dataset information\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            # Print statistics for soil moisture variables\n",
    "            print(\"\\nSoil Moisture Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable attributes\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                # Calculate statistics for numeric data\n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving NLDAS soil moisture data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDAS data retrieval with progress tracking\n",
    "def validate_dates(start_date: str, end_date: str) -> tuple:\n",
    "    \"\"\"Validate and parse date strings.\"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        if end < start:\n",
    "            raise ValueError(\"End date must be after start date\")\n",
    "        return start, end\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Please use YYYY-MM-DD format. Error: {str(e)}\")\n",
    "\n",
    "def validate_bounds(lat_bounds: tuple, lon_bounds: tuple) -> None:\n",
    "    \"\"\"Validate spatial bounds for GLDAS data.\"\"\"\n",
    "    if not isinstance(lat_bounds, tuple) or not isinstance(lon_bounds, tuple):\n",
    "        raise TypeError(\"Bounds must be tuples\")\n",
    "    if len(lat_bounds) != 2 or len(lon_bounds) != 2:\n",
    "        raise ValueError(\"Bounds must contain exactly 2 values\")\n",
    "    if not (-60 <= lat_bounds[0] <= 90 and -60 <= lat_bounds[1] <= 90):\n",
    "        raise ValueError(\"GLDAS latitude must be between -60 and 90 degrees\")\n",
    "    if not (-180 <= lon_bounds[0] <= 180 and -180 <= lon_bounds[1] <= 180):\n",
    "        raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "    if lat_bounds[0] >= lat_bounds[1]:\n",
    "        raise ValueError(\"Minimum latitude must be less than maximum latitude\")\n",
    "    if lon_bounds[0] >= lon_bounds[1]:\n",
    "        raise ValueError(\"Minimum longitude must be less than maximum longitude\")\n",
    "\n",
    "def export_dataset(ds: xr.Dataset, start_date: str, end_date: str, output_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export dataset to NetCDF file with date-specific filename.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset to export\n",
    "    start_date : str\n",
    "        Start date string\n",
    "    end_date : str\n",
    "        End date string\n",
    "    output_dir : str, optional\n",
    "        Directory to save the file (default: current directory)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the exported file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = '.'\n",
    "        \n",
    "    # Create filename\n",
    "    filename = f\"gldas_data_{start_date}_{end_date}.nc\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add export metadata\n",
    "    ds.attrs['export_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds.attrs['data_period'] = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    # Export to NetCDF\n",
    "    print(f\"\\nExporting data to {filepath}...\")\n",
    "    ds.to_netcdf(filepath)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def get_gldas_data(start_date: str, end_date: str, lat_bounds: tuple, lon_bounds: tuple, \n",
    "                   output_dir: str = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieve GLDAS soil moisture data with progress tracking and validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest (between -60 and 90)\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest (-180 to 180)\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output file (default: current directory)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with GLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_dates(start_date, end_date)\n",
    "        validate_bounds(lat_bounds, lon_bounds)\n",
    "        \n",
    "        print(\"\\nRetrieving GLDAS soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        try:\n",
    "            auth = earthaccess.login()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to authenticate with NASA Earthdata: {str(e)}\")\n",
    "        \n",
    "        # Search for granules with retry mechanism\n",
    "        max_retries = 3\n",
    "        granules = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nSearching for GLDAS granules (attempt {attempt + 1}/{max_retries})...\")\n",
    "                granules = earthaccess.search_data(\n",
    "                    short_name=\"GLDAS_NOAH025_M\",\n",
    "                    version=\"2.1\",\n",
    "                    temporal=(start_date, end_date),\n",
    "                    bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "                )\n",
    "                if granules:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise RuntimeError(f\"Failed to search for granules after {max_retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                continue\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No GLDAS granules found for the specified parameters\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        # Process data in temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files with progress tracking\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise RuntimeError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files with detailed error handling\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            failed_files = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print dimensions and variables for first file\n",
    "                    if processed_count == 0:\n",
    "                        print(f\"\\nFile structure: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Available variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Validate dataset structure\n",
    "                    required_dims = {'time', 'lat', 'lon'}\n",
    "                    if not all(dim in ds.dims for dim in required_dims):\n",
    "                        raise ValueError(f\"Missing required dimensions: {required_dims - set(ds.dims)}\")\n",
    "                    \n",
    "                    # Select soil moisture variables\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting with validation\n",
    "                    ds = ds.sel(\n",
    "                        lat=slice(lat_bounds[0], lat_bounds[1]),\n",
    "                        lon=slice(lon_bounds[0], lon_bounds[1])\n",
    "                    )\n",
    "                    \n",
    "                    # Validate data content\n",
    "                    if ds.sizes['lat'] == 0 or ds.sizes['lon'] == 0:\n",
    "                        raise ValueError(\"No data points within specified bounds\")\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed_files.append((os.path.basename(file_path), str(e)))\n",
    "                    continue\n",
    "            \n",
    "            # Report processing results\n",
    "            if failed_files:\n",
    "                print(\"\\nWarning: Some files failed to process:\")\n",
    "                for fname, error in failed_files:\n",
    "                    print(f\"  - {fname}: {error}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets with error handling\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time')\n",
    "                combined_ds = combined_ds.sortby('time')  # Ensure temporal ordering\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to combine datasets: {str(e)}\")\n",
    "            \n",
    "            # Generate comprehensive dataset report\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Time resolution: {np.median(np.diff(combined_ds.time.values)).astype('timedelta64[h]')}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Spatial coverage: {combined_ds.sizes['lat']}x{combined_ds.sizes['lon']} grid points\")\n",
    "            print(f\"Lat range: {float(combined_ds.lat.min().values):.3f} to {float(combined_ds.lat.max().values):.3f}\")\n",
    "            print(f\"Lon range: {float(combined_ds.lon.min().values):.3f} to {float(combined_ds.lon.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate and report statistics for each layer\n",
    "            print(\"\\nSoil Moisture Statistics by Layer:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable metadata\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min (0th):   {percentiles[0]:.4f}\")\n",
    "                        print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                        print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                        print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                        print(f\"  Max (100th):  {percentiles[4]:.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                        \n",
    "                        # Check for potentially anomalous values\n",
    "                        q1, q3 = percentiles[1], percentiles[3]\n",
    "                        iqr = q3 - q1\n",
    "                        outliers = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))\n",
    "                        if outliers > 0:\n",
    "                            print(f\"  Potential outliers: {outliers:,} points\")\n",
    "                    else:\n",
    "                        print(\"Warning: No valid numeric data found\")\n",
    "            \n",
    "            # Export the dataset\n",
    "            export_filepath = export_dataset(combined_ds, start_date, end_date, output_dir)\n",
    "            print(f\"\\nData exported to: {export_filepath}\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nError retrieving GLDAS soil moisture data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLDAS data retrieval with progress tracking\n",
    "def validate_dates(start_date: str, end_date: str) -> tuple:\n",
    "    \"\"\"Validate and parse date strings.\"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        if end < start:\n",
    "            raise ValueError(\"End date must be after start date\")\n",
    "        return start, end\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Please use YYYY-MM-DD format. Error: {str(e)}\")\n",
    "\n",
    "def validate_bounds(lat_bounds: tuple, lon_bounds: tuple) -> None:\n",
    "    \"\"\"Validate spatial bounds for FLDAS data.\"\"\"\n",
    "    if not isinstance(lat_bounds, tuple) or not isinstance(lon_bounds, tuple):\n",
    "        raise TypeError(\"Bounds must be tuples\")\n",
    "    if len(lat_bounds) != 2 or len(lon_bounds) != 2:\n",
    "        raise ValueError(\"Bounds must contain exactly 2 values\")\n",
    "    if not (-60 <= lat_bounds[0] <= 90 and -60 <= lat_bounds[1] <= 90):\n",
    "        raise ValueError(\"FLDAS latitude must be between -60 and 90 degrees\")\n",
    "    if not (-180 <= lon_bounds[0] <= 180 and -180 <= lon_bounds[1] <= 180):\n",
    "        raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "    if lat_bounds[0] >= lat_bounds[1]:\n",
    "        raise ValueError(\"Minimum latitude must be less than maximum latitude\")\n",
    "    if lon_bounds[0] >= lon_bounds[1]:\n",
    "        raise ValueError(\"Minimum longitude must be less than maximum longitude\")\n",
    "\n",
    "def export_dataset(ds: xr.Dataset, start_date: str, end_date: str, output_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export dataset to NetCDF file with date-specific filename.\n",
    "    \"\"\"\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = '.'\n",
    "        \n",
    "    filename = f\"fldas_data_{start_date}_{end_date}.nc\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add export metadata\n",
    "    ds.attrs['export_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds.attrs['data_period'] = f\"{start_date} to {end_date}\"\n",
    "    ds.attrs['data_source'] = \"FLDAS_NOAH01_C_GL_M.001\"\n",
    "    \n",
    "    print(f\"\\nExporting data to {filepath}...\")\n",
    "    ds.to_netcdf(filepath)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def get_fldas_data(start_date: str, end_date: str, lat_bounds: tuple, lon_bounds: tuple, \n",
    "                   output_dir: str = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieve FLDAS soil moisture data with progress tracking, validation, and export.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output file (default: current directory)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with FLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_dates(start_date, end_date)\n",
    "        validate_bounds(lat_bounds, lon_bounds)\n",
    "        \n",
    "        print(\"\\nRetrieving FLDAS soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        try:\n",
    "            auth = earthaccess.login()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to authenticate with NASA Earthdata: {str(e)}\")\n",
    "        \n",
    "        # Search for granules with retry mechanism\n",
    "        max_retries = 3\n",
    "        granules = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nSearching for FLDAS granules (attempt {attempt + 1}/{max_retries})...\")\n",
    "                granules = earthaccess.search_data(\n",
    "                    short_name=\"FLDAS_NOAH01_C_GL_M\",\n",
    "                    version=\"001\",\n",
    "                    temporal=(start_date, end_date),\n",
    "                    bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "                )\n",
    "                if granules:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise RuntimeError(f\"Failed to search for granules after {max_retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                continue\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No FLDAS granules found for the specified parameters\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise RuntimeError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            failed_files = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print information for first file\n",
    "                    if processed_count == 0:\n",
    "                        print(f\"\\nFile structure: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Available variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Select soil moisture variables\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var and 'cm_tavg' in var]\n",
    "                    if not soil_vars:\n",
    "                        raise ValueError(f\"No soil moisture variables found\")\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Handle FLDAS specific coordinate system\n",
    "                    if 'X' in ds.dims and 'Y' in ds.dims:\n",
    "                        ds = ds.sel(\n",
    "                            X=slice(lon_bounds[0], lon_bounds[1]),\n",
    "                            Y=slice(lat_bounds[0], lat_bounds[1])\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Expected X/Y coordinates not found in dataset\")\n",
    "                    \n",
    "                    # Validate data content\n",
    "                    if ds.sizes['X'] == 0 or ds.sizes['Y'] == 0:\n",
    "                        raise ValueError(\"No data points within specified bounds\")\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed_files.append((os.path.basename(file_path), str(e)))\n",
    "                    continue\n",
    "            \n",
    "            # Report processing results\n",
    "            if failed_files:\n",
    "                print(\"\\nWarning: Some files failed to process:\")\n",
    "                for fname, error in failed_files:\n",
    "                    print(f\"  - {fname}: {error}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time')\n",
    "                combined_ds = combined_ds.sortby('time')\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to combine datasets: {str(e)}\")\n",
    "            \n",
    "            # Dataset report\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Time resolution: {np.median(np.diff(combined_ds.time.values)).astype('timedelta64[h]')}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Spatial coverage: {combined_ds.sizes['Y']}x{combined_ds.sizes['X']} grid points\")\n",
    "            print(f\"Y range: {float(combined_ds.Y.min().values):.3f} to {float(combined_ds.Y.max().values):.3f}\")\n",
    "            print(f\"X range: {float(combined_ds.X.min().values):.3f} to {float(combined_ds.X.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate statistics by layer\n",
    "            print(\"\\nSoil Moisture Statistics by Layer:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get metadata\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min (0th):   {percentiles[0]:.4f}\")\n",
    "                        print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                        print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                        print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                        print(f\"  Max (100th):  {percentiles[4]:.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                        \n",
    "                        # Check for outliers\n",
    "                        q1, q3 = percentiles[1], percentiles[3]\n",
    "                        iqr = q3 - q1\n",
    "                        outliers = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))\n",
    "                        if outliers > 0:\n",
    "                            print(f\"  Potential outliers: {outliers:,} points\")\n",
    "                    else:\n",
    "                        print(\"Warning: No valid numeric data found\")\n",
    "            \n",
    "            # Export the dataset\n",
    "            export_filepath = export_dataset(combined_ds, start_date, end_date, output_dir)\n",
    "            print(f\"\\nData exported to: {export_filepath}\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nError retrieving FLDAS soil moisture data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving FLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for FLDAS granules (attempt 1/3)...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 17053.69it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [31:53<00:00, 14.49s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 146158.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 9/132 [00:00<00:05, 23.01file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure: FLDAS_NOAH01_C_GL_M.A201301.001.nc\n",
      "Dimensions: ['time', 'bnds', 'X', 'Y']\n",
      "Available variables: ['time_bnds', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'Psurf_f_tavg', 'Qair_f_tavg', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_tavg', 'Qsb_tavg', 'RadT_tavg', 'Rainf_f_tavg', 'SWE_inst', 'SWdown_f_tavg', 'SnowCover_inst', 'SnowDepth_inst', 'Snowf_tavg', 'Swnet_tavg', 'Tair_f_tavg', 'Wind_f_tavg', 'SoilMoi00_10cm_tavg', 'SoilMoi10_40cm_tavg', 'SoilMoi40_100cm_tavg', 'SoilMoi100_200cm_tavg', 'SoilTemp00_10cm_tavg', 'SoilTemp10_40cm_tavg', 'SoilTemp40_100cm_tavg', 'SoilTemp100_200cm_tavg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:02<00:00, 65.47file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Time resolution: 744 hours\n",
      "Number of timesteps: 132\n",
      "Spatial coverage: 45x79 grid points\n",
      "Y range: 40.550 to 44.950\n",
      "X range: -79.750 to -71.950\n",
      "\n",
      "Soil Moisture Statistics by Layer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilMoi00_10cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3622\n",
      "  Std Dev:  0.0497\n",
      "  Min (0th):   0.1411\n",
      "  25th:     0.3361\n",
      "  Median:   0.3690\n",
      "  75th:     0.3959\n",
      "  Max (100th):  0.4678\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,172 points\n",
      "\n",
      "Variable: SoilMoi10_40cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3800\n",
      "  Std Dev:  0.0672\n",
      "  Min (0th):   0.0897\n",
      "  25th:     0.3452\n",
      "  Median:   0.3867\n",
      "  75th:     0.4292\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,134 points\n",
      "\n",
      "Variable: SoilMoi40_100cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3456\n",
      "  Std Dev:  0.0804\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3084\n",
      "  Median:   0.3695\n",
      "  75th:     0.4006\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 23,405 points\n",
      "\n",
      "Variable: SoilMoi100_200cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3592\n",
      "  Std Dev:  0.0597\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3327\n",
      "  Median:   0.3670\n",
      "  75th:     0.3973\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 20,378 points\n",
      "\n",
      "Exporting data to fldas_data/fldas_data_2013-01-01_2023-12-31.nc...\n",
      "Export complete!\n",
      "\n",
      "Data exported to: fldas_data/fldas_data_2013-01-01_2023-12-31.nc\n",
      "\n",
      "Data retrieval successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_dates(start_date: str, end_date: str) -> tuple:\n",
    "    \"\"\"Validate and parse date strings.\"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        if end < start:\n",
    "            raise ValueError(\"End date must be after start date\")\n",
    "        return start, end\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Please use YYYY-MM-DD format. Error: {str(e)}\")\n",
    "\n",
    "def validate_bounds(lat_bounds: tuple, lon_bounds: tuple) -> None:\n",
    "    \"\"\"Validate spatial bounds for FLDAS data.\"\"\"\n",
    "    if not isinstance(lat_bounds, tuple) or not isinstance(lon_bounds, tuple):\n",
    "        raise TypeError(\"Bounds must be tuples\")\n",
    "    if len(lat_bounds) != 2 or len(lon_bounds) != 2:\n",
    "        raise ValueError(\"Bounds must contain exactly 2 values\")\n",
    "    if not (-60 <= lat_bounds[0] <= 90 and -60 <= lat_bounds[1] <= 90):\n",
    "        raise ValueError(\"FLDAS latitude must be between -60 and 90 degrees\")\n",
    "    if not (-180 <= lon_bounds[0] <= 180 and -180 <= lon_bounds[1] <= 180):\n",
    "        raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "    if lat_bounds[0] >= lat_bounds[1]:\n",
    "        raise ValueError(\"Minimum latitude must be less than maximum latitude\")\n",
    "    if lon_bounds[0] >= lon_bounds[1]:\n",
    "        raise ValueError(\"Minimum longitude must be less than maximum longitude\")\n",
    "\n",
    "def export_dataset(ds: xr.Dataset, start_date: str, end_date: str, output_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export dataset to NetCDF file with date-specific filename.\n",
    "    \"\"\"\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = '.'\n",
    "        \n",
    "    filename = f\"fldas_data_{start_date}_{end_date}.nc\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add export metadata\n",
    "    ds.attrs['export_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds.attrs['data_period'] = f\"{start_date} to {end_date}\"\n",
    "    ds.attrs['data_source'] = \"FLDAS_NOAH01_C_GL_M.001\"\n",
    "    \n",
    "    print(f\"\\nExporting data to {filepath}...\")\n",
    "    ds.to_netcdf(filepath)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def get_fldas_data(start_date: str, end_date: str, lat_bounds: tuple, lon_bounds: tuple, \n",
    "                   output_dir: str = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieve FLDAS soil moisture data with progress tracking, validation, and export.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output file (default: current directory)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with FLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_dates(start_date, end_date)\n",
    "        validate_bounds(lat_bounds, lon_bounds)\n",
    "        \n",
    "        print(\"\\nRetrieving FLDAS soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        try:\n",
    "            auth = earthaccess.login()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to authenticate with NASA Earthdata: {str(e)}\")\n",
    "        \n",
    "        # Search for granules with retry mechanism\n",
    "        max_retries = 3\n",
    "        granules = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nSearching for FLDAS granules (attempt {attempt + 1}/{max_retries})...\")\n",
    "                granules = earthaccess.search_data(\n",
    "                    short_name=\"FLDAS_NOAH01_C_GL_M\",\n",
    "                    version=\"001\",\n",
    "                    temporal=(start_date, end_date),\n",
    "                    bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "                )\n",
    "                if granules:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise RuntimeError(f\"Failed to search for granules after {max_retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                continue\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No FLDAS granules found for the specified parameters\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise RuntimeError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            failed_files = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print information for first file\n",
    "                    if processed_count == 0:\n",
    "                        print(f\"\\nFile structure: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Available variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Select soil moisture variables\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var and 'cm_tavg' in var]\n",
    "                    if not soil_vars:\n",
    "                        raise ValueError(f\"No soil moisture variables found\")\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Handle FLDAS specific coordinate system\n",
    "                    if 'X' in ds.dims and 'Y' in ds.dims:\n",
    "                        ds = ds.sel(\n",
    "                            X=slice(lon_bounds[0], lon_bounds[1]),\n",
    "                            Y=slice(lat_bounds[0], lat_bounds[1])\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Expected X/Y coordinates not found in dataset\")\n",
    "                    \n",
    "                    # Validate data content\n",
    "                    if ds.sizes['X'] == 0 or ds.sizes['Y'] == 0:\n",
    "                        raise ValueError(\"No data points within specified bounds\")\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed_files.append((os.path.basename(file_path), str(e)))\n",
    "                    continue\n",
    "            \n",
    "            # Report processing results\n",
    "            if failed_files:\n",
    "                print(\"\\nWarning: Some files failed to process:\")\n",
    "                for fname, error in failed_files:\n",
    "                    print(f\"  - {fname}: {error}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time')\n",
    "                combined_ds = combined_ds.sortby('time')\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to combine datasets: {str(e)}\")\n",
    "            \n",
    "            # Dataset report\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Time resolution: {np.median(np.diff(combined_ds.time.values)).astype('timedelta64[h]')}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Spatial coverage: {combined_ds.sizes['Y']}x{combined_ds.sizes['X']} grid points\")\n",
    "            print(f\"Y range: {float(combined_ds.Y.min().values):.3f} to {float(combined_ds.Y.max().values):.3f}\")\n",
    "            print(f\"X range: {float(combined_ds.X.min().values):.3f} to {float(combined_ds.X.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate statistics by layer\n",
    "            print(\"\\nSoil Moisture Statistics by Layer:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get metadata\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min (0th):   {percentiles[0]:.4f}\")\n",
    "                        print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                        print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                        print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                        print(f\"  Max (100th):  {percentiles[4]:.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                        \n",
    "                        # Check for outliers\n",
    "                        q1, q3 = percentiles[1], percentiles[3]\n",
    "                        iqr = q3 - q1\n",
    "                        outliers = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))\n",
    "                        if outliers > 0:\n",
    "                            print(f\"  Potential outliers: {outliers:,} points\")\n",
    "                    else:\n",
    "                        print(\"Warning: No valid numeric data found\")\n",
    "            \n",
    "            # Export the dataset\n",
    "            export_filepath = export_dataset(combined_ds, start_date, end_date, output_dir)\n",
    "            print(f\"\\nData exported to: {export_filepath}\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nError retrieving FLDAS soil moisture data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    start_date = \"2013-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    lat_bounds = (min_lat, max_lat)\n",
    "    lon_bounds = (min_lon, max_lon)\n",
    "    output_dir = \"fldas_data\"\n",
    "    \n",
    "    # Retrieve data\n",
    "    try:\n",
    "        ds = get_fldas_data(start_date, end_date, lat_bounds, lon_bounds, output_dir)\n",
    "        print(\"\\nData retrieval successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAP data retrieval with progress tracking\n",
    "def get_smap_data(start_date, end_date, lat_bounds, lon_bounds, max_files=10):\n",
    "    \"\"\"\n",
    "    Retrieve SMAP L4 soil moisture data with missing value handling\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving SMAP soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        granules = earthaccess.search_data(\n",
    "            count=max_files,\n",
    "            short_name=\"SPL4SMGP\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No SMAP granules found\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download granules\")\n",
    "                \n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing\"):\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as f:\n",
    "                        if len(datasets) == 0:\n",
    "                            print(\"\\nFile structure:\")\n",
    "                            def print_structure(name, obj):\n",
    "                                if isinstance(obj, h5py.Dataset):\n",
    "                                    print(f\"{name}:\")\n",
    "                                    print(f\"  Shape: {obj.shape}\")\n",
    "                                    print(f\"  Dtype: {obj.dtype}\")\n",
    "                                    if '_FillValue' in obj.attrs:\n",
    "                                        print(f\"  Fill Value: {obj.attrs['_FillValue']}\")\n",
    "                            f.visititems(print_structure)\n",
    "                        \n",
    "                        if 'Geophysical_Data' in f:\n",
    "                            geo_data = f['Geophysical_Data']\n",
    "                            sm_vars = ['sm_surface', 'sm_rootzone', 'sm_profile']\n",
    "                            ds_dict = {}\n",
    "                            \n",
    "                            time_value = None\n",
    "                            for attr in f.attrs.keys():\n",
    "                                if 'time' in attr.lower():\n",
    "                                    try:\n",
    "                                        time_str = f.attrs[attr]\n",
    "                                        if isinstance(time_str, bytes):\n",
    "                                            time_str = time_str.decode('utf-8')\n",
    "                                        time_value = pd.to_datetime(time_str)\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        continue\n",
    "                            \n",
    "                            if time_value is None:\n",
    "                                time_value = pd.Timestamp(start_date)\n",
    "                            \n",
    "                            for var in sm_vars:\n",
    "                                if var in geo_data:\n",
    "                                    # Get the data and attributes\n",
    "                                    data = geo_data[var][:]\n",
    "                                    attrs = dict(geo_data[var].attrs)\n",
    "                                    \n",
    "                                    # Handle missing values\n",
    "                                    # Check for _FillValue in attributes\n",
    "                                    fill_value = attrs.get('_FillValue', -9999)\n",
    "                                    # Replace both -9999 and the fill_value with NaN\n",
    "                                    data = np.where(data == -9999, np.nan, data)\n",
    "                                    if fill_value != -9999:\n",
    "                                        data = np.where(data == fill_value, np.nan, data)\n",
    "                                    \n",
    "                                    y_size, x_size = data.shape\n",
    "                                    coords = {\n",
    "                                        'y': np.linspace(lat_bounds[0], lat_bounds[1], y_size),\n",
    "                                        'x': np.linspace(lon_bounds[0], lon_bounds[1], x_size),\n",
    "                                        'time': [time_value]\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Print statistics for this variable\n",
    "                                    print(f\"\\nStatistics for {var}:\")\n",
    "                                    valid_data = data[~np.isnan(data)]\n",
    "                                    if len(valid_data) > 0:\n",
    "                                        print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                                        print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                                        print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                                        print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                                        print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                                        print(f\"  Units:           {attrs.get('units', 'unknown')}\")\n",
    "                                    \n",
    "                                    da = xr.DataArray(\n",
    "                                        data[np.newaxis, :, :],\n",
    "                                        dims=['time', 'y', 'x'],\n",
    "                                        coords=coords,\n",
    "                                        name=var,\n",
    "                                        attrs=attrs\n",
    "                                    )\n",
    "                                    ds_dict[var] = da\n",
    "                            \n",
    "                            if ds_dict:\n",
    "                                ds = xr.Dataset(ds_dict)\n",
    "                                datasets.append(ds)\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process {os.path.basename(file_path)}: {str(e)}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found\")\n",
    "            \n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            print(\"\\nRetrieved data summary:\")\n",
    "            print(f\"Time period: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(\"Variables:\", list(combined_ds.data_vars))\n",
    "            \n",
    "            # Print final statistics for combined dataset\n",
    "            print(\"\\nFinal Dataset Statistics:\")\n",
    "            for var in combined_ds.data_vars:\n",
    "                data = combined_ds[var].values\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                print(f\"\\n{var}:\")\n",
    "                if len(valid_data) > 0:\n",
    "                    print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                    print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                    print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                    print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                    print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                    print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                    print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERRA2-data\n",
    "def get_merra2_monthly_soil_moisture(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    lat_bounds: Tuple[float, float],\n",
    "    lon_bounds: Tuple[float, float],\n",
    "    output_dir: str = None,\n",
    "    var_names: List[str] = None\n",
    ") -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Retrieve MERRA-2 monthly soil moisture data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    output_dir : str, optional\n",
    "        Directory to save output files\n",
    "    var_names : list, optional\n",
    "        List of variable names to retrieve. Defaults to:\n",
    "        - SFMC: Surface Soil Moisture Content (0-5 cm)\n",
    "        - RZMC: Root Zone Soil Moisture Content (0-100 cm)\n",
    "        - PRMC: Profile Soil Moisture Content (0-200 cm)\n",
    "        - GWETTOP: Surface Soil Wetness\n",
    "        - GWETROOT: Root Zone Soil Wetness\n",
    "        - GWETPROF: Profile Soil Wetness\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with monthly soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set default variables if not specified\n",
    "        if var_names is None:\n",
    "            var_names = ['SFMC', 'RZMC', 'PRMC', 'GWETTOP', 'GWETROOT', 'GWETPROF']\n",
    "        \n",
    "        # Parse dates\n",
    "        try:\n",
    "            start = datetime.strptime(start_date, '%Y-%m')\n",
    "            end = datetime.strptime(end_date, '%Y-%m')\n",
    "            if end < start:\n",
    "                raise ValueError(\"End date must be after start date\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid date format. Please use YYYY-MM format. Error: {str(e)}\")\n",
    "        \n",
    "        # Validate bounds\n",
    "        minlon, minlat = lon_bounds[0], lat_bounds[0]\n",
    "        maxlon, maxlat = lon_bounds[1], lat_bounds[1]\n",
    "        \n",
    "        if not (-180 <= minlon <= 180 and -180 <= maxlon <= 180):\n",
    "            raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "        if not (-90 <= minlat <= 90 and -90 <= maxlat <= 90):\n",
    "            raise ValueError(\"Latitude must be between -90 and 90 degrees\")\n",
    "        if minlon >= maxlon or minlat >= maxlat:\n",
    "            raise ValueError(\"Min values must be less than max values\")\n",
    "        \n",
    "        print(\"\\nRetrieving MERRA-2 monthly soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat [{minlat}, {maxlat}], lon [{minlon}, {maxlon}]\")\n",
    "        print(f\"Variables: {var_names}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        # Search for granules\n",
    "        print(\"\\nSearching for data files...\")\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"M2TMNXLND\",  # Monthly mean land surface diagnostics\n",
    "            version=\"5.12.4\",\n",
    "            temporal=(f\"{start_date}-01\", f\"{end_date}-01\"),\n",
    "            bounding_box=(minlon, minlat, maxlon, maxlat)\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No MERRA-2 monthly data found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} files\")\n",
    "        \n",
    "        # Process files\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading files...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir,\n",
    "                threads=4\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any files\")\n",
    "                \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process downloaded files\n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Select requested variables\n",
    "                    available_vars = [var for var in var_names if var in ds.data_vars]\n",
    "                    if not available_vars:\n",
    "                        print(f\"Warning: No requested variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[available_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting\n",
    "                    ds = ds.sel(\n",
    "                        lon=slice(minlon, maxlon),\n",
    "                        lat=slice(minlat, maxlat)\n",
    "                    )\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid data found after processing\")\n",
    "            \n",
    "            # Combine all datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            ds = xr.concat(datasets, dim='time')\n",
    "            ds = ds.sortby('time')\n",
    "            \n",
    "            # Add metadata\n",
    "            ds.attrs.update({\n",
    "                'title': 'MERRA-2 Monthly Soil Moisture Data',\n",
    "                'source': 'M2TMNXLND.5.12.4',\n",
    "                'time_coverage_start': start_date,\n",
    "                'time_coverage_end': end_date,\n",
    "                'geospatial_lat_min': float(ds.lat.min()),\n",
    "                'geospatial_lat_max': float(ds.lat.max()),\n",
    "                'geospatial_lon_min': float(ds.lon.min()),\n",
    "                'geospatial_lon_max': float(ds.lon.max())\n",
    "            })\n",
    "            \n",
    "            # Add variable descriptions\n",
    "            var_descriptions = {\n",
    "                'SFMC': 'Surface Soil Moisture Content (0-5 cm)',\n",
    "                'RZMC': 'Root Zone Soil Moisture Content (0-100 cm)',\n",
    "                'PRMC': 'Profile Soil Moisture Content (0-200 cm)',\n",
    "                'GWETTOP': 'Surface Soil Wetness',\n",
    "                'GWETROOT': 'Root Zone Soil Wetness',\n",
    "                'GWETPROF': 'Profile Soil Wetness'\n",
    "            }\n",
    "            \n",
    "            for var in ds.data_vars:\n",
    "                if var in var_descriptions:\n",
    "                    ds[var].attrs['long_name'] = var_descriptions[var]\n",
    "            \n",
    "            # Print dataset information\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "            print(f\"Number of months: {len(ds.time)}\")\n",
    "            print(f\"Spatial coverage: {ds.sizes['lat']}x{ds.sizes['lon']} grid points\")\n",
    "            print(f\"Lat range: {float(ds.lat.min().values):.3f} to {float(ds.lat.max().values):.3f}\")\n",
    "            print(f\"Lon range: {float(ds.lon.min().values):.3f} to {float(ds.lon.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            print(\"\\nVariable Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                if var in var_descriptions:\n",
    "                    print(f\"Description: {var_descriptions[var]}\")\n",
    "                \n",
    "                data = ds[var].values\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                \n",
    "                if len(valid_data) > 0:\n",
    "                    percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                    print(\"Statistics:\")\n",
    "                    print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                    print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                    print(f\"  Min:      {percentiles[0]:.4f}\")\n",
    "                    print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                    print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                    print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                    print(f\"  Max:      {percentiles[4]:.4f}\")\n",
    "                    print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                    print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                    print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            \n",
    "            # Export dataset\n",
    "            if output_dir:\n",
    "                Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, \n",
    "                    f\"merra2_monthly_soil_moisture_{start_date}_{end_date}.nc\")\n",
    "                print(f\"\\nExporting data to {output_file}\")\n",
    "                \n",
    "                # Add compression\n",
    "                encoding = {var: {'zlib': True, 'complevel': 5} for var in ds.data_vars}\n",
    "                ds.to_netcdf(output_file, encoding=encoding)\n",
    "                print(\"Export complete!\")\n",
    "            \n",
    "            return ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving MERRA-2 monthly soil moisture data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving ERA5 data...\n",
      "Dataset: reanalysis-era5-land\n",
      "Time range: 2023-01\n",
      "Spatial bounds: (45.02, -79.77, 40.5, -71.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 09:21:50,741 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-04 09:21:50,742 WARNING [2024-09-26T00:00:00] Should you have not yet migrated from the old CDS system to the new CDS, please check our [informative page](https://confluence.ecmwf.int/x/uINmFw) for guidance.\n",
      "2024-11-04 09:21:50,742 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-04 09:21:50,743 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-04 09:21:50,744 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting request to ERA5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 09:21:51,049 INFO Request ID is 7ebeeeb5-8b22-4f63-95f0-5b5c776da6fd\n",
      "2024-11-04 09:21:51,554 INFO status has been updated to accepted\n",
      "2024-11-04 09:21:56,960 INFO status has been updated to successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete. File size: 0.28 MB\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Dimensions: {'valid_time': 48, 'latitude': 46, 'longitude': 80}\n",
      "\n",
      "Variables:\n",
      "\n",
      "Variable: swvl1\n",
      "Description: Volumetric soil water layer 1\n",
      "Units: m**3 m**-3\n",
      "Shape: (48, 46, 80)\n",
      "Statistics:\n",
      "  Mean:     0.3555\n",
      "  Median:   0.3956\n",
      "  Std Dev:  0.0937\n",
      "  Variance: 0.0088\n",
      "  Min:      0.0100\n",
      "  Max:      0.5200\n",
      "  Valid Points:    170,784\n",
      "  Missing Points:  5,856\n",
      "  Data Coverage:   96.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/0tkgm2g11c9202ygt6n5r_qh0000gn/T/ipykernel_61105/1923655684.py:48: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# ERA5_land request:\n",
    "request = {\n",
    "    \"variable\": \"volumetric_soil_water_layer_1\",\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"year\": \"2023\",\n",
    "    \"month\": \"01\",\n",
    "    \"day\": [\"01\", \"02\"],\n",
    "    \"time\": [f\"{hour:02d}:00\" for hour in range(24)],\n",
    "    \"area\": area,  # [north, west, south, east]\n",
    "    \"format\": \"netcdf\"\n",
    " }\n",
    "    \n",
    "    # Get the data\n",
    "try:\n",
    "     era5_data = get_era5_data(\n",
    "        dataset=\"reanalysis-era5-land\",\n",
    "        request=request,\n",
    "        output_file=\"era5_soil_moisture.nc\"\n",
    "    )  \n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve ERA5 data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving NLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for NLDAS granules...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 17833.15it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [03:02<00:00,  1.38s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 323015.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:03<00:00, 37.72file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Number of timesteps: 132\n",
      "Dimensions: {'time': 132, 'lat': 36, 'lon': 63}\n",
      "\n",
      "Soil Moisture Statistics:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilM_0_10cm\n",
      "Description: Soil moisture content (0-10cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     29.3776\n",
      "  Median:   28.3492\n",
      "  Std Dev:  7.0062\n",
      "  Min:      2.9674\n",
      "  Max:      47.5999\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_10_40cm\n",
      "Description: Soil moisture content (10-40cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     88.4562\n",
      "  Median:   85.0910\n",
      "  Std Dev:  20.4660\n",
      "  Min:      16.7697\n",
      "  Max:      142.7994\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_40_100cm\n",
      "Description: Soil moisture content (40-100cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     163.7207\n",
      "  Median:   164.3904\n",
      "  Std Dev:  29.5161\n",
      "  Min:      12.0045\n",
      "  Max:      282.3860\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_100_200cm\n",
      "Description: Soil moisture content (100-200cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     263.4715\n",
      "  Median:   269.7338\n",
      "  Std Dev:  46.3734\n",
      "  Min:      20.0000\n",
      "  Max:      456.5619\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_0_100cm\n",
      "Description: Soil moisture content (0-100cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     281.5546\n",
      "  Median:   283.5311\n",
      "  Std Dev:  48.8255\n",
      "  Min:      51.5873\n",
      "  Max:      448.6649\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_0_200cm\n",
      "Description: Soil moisture content (0-200cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     545.0261\n",
      "  Median:   556.9470\n",
      "  Std Dev:  83.3693\n",
      "  Min:      77.1631\n",
      "  Max:      840.6978\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Saving data to nldas_data_2013-01-01_2023-12-31.nc...\n",
      "File saved successfully. Size: 4.60 MB\n"
     ]
    }
   ],
   "source": [
    "# Example usage NLDAS:\n",
    "if __name__ == \"__main__\":\n",
    "    start_date = start_date\n",
    "    end_date = end_date\n",
    "    lat_bounds = lat_bounds\n",
    "    lon_bounds = lon_bounds\n",
    "    \n",
    "    try:\n",
    "        # Set up output file name\n",
    "        output_file = f\"nldas_data_{start_date}_{end_date}.nc\"\n",
    "        \n",
    "        # Retrieve the data\n",
    "        nldas_data = get_nldas_data(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            lat_bounds=lat_bounds,\n",
    "            lon_bounds=lon_bounds\n",
    "        )\n",
    "        \n",
    "        # Save the data\n",
    "        print(f\"\\nSaving data to {output_file}...\")\n",
    "        nldas_data.to_netcdf(output_file)\n",
    "        \n",
    "        # Print file size\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"File saved successfully. Size: {file_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve NLDAS data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving GLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for GLDAS granules (attempt 1/3)...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 16019.45it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [06:58<00:00,  3.17s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 296068.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/132 [00:00<00:22,  5.75file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure: GLDAS_NOAH025_M.A201301.021.nc4\n",
      "Dimensions: ['time', 'bnds', 'lon', 'lat']\n",
      "Available variables: ['time_bnds', 'Swnet_tavg', 'Lwnet_tavg', 'Qle_tavg', 'Qh_tavg', 'Qg_tavg', 'Snowf_tavg', 'Rainf_tavg', 'Evap_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'AvgSurfT_inst', 'Albedo_inst', 'SWE_inst', 'SnowDepth_inst', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'SoilTMP100_200cm_inst', 'PotEvap_tavg', 'ECanop_tavg', 'Tveg_tavg', 'ESoil_tavg', 'RootMoist_inst', 'CanopInt_inst', 'Wind_f_inst', 'Rainf_f_tavg', 'Tair_f_inst', 'Qair_f_inst', 'Psurf_f_inst', 'SWdown_f_tavg', 'LWdown_f_tavg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:02<00:00, 53.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Time resolution: 744 hours\n",
      "Number of timesteps: 132\n",
      "Spatial coverage: 18x32 grid points\n",
      "Lat range: 40.625 to 44.875\n",
      "Lon range: -79.625 to -71.875\n",
      "\n",
      "Soil Moisture Statistics by Layer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilMoi0_10cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     31.4795\n",
      "  Std Dev:  6.4977\n",
      "  Min (0th):   8.3211\n",
      "  25th:     26.7874\n",
      "  Median:   30.1789\n",
      "  75th:     34.7768\n",
      "  Max (100th):  47.5826\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 1,444 points\n",
      "\n",
      "Variable: SoilMoi10_40cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     95.2638\n",
      "  Std Dev:  21.2855\n",
      "  Min (0th):   20.6895\n",
      "  25th:     80.3381\n",
      "  Median:   90.7238\n",
      "  75th:     107.9240\n",
      "  Max (100th):  142.7901\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 190 points\n",
      "\n",
      "Variable: SoilMoi40_100cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     174.2118\n",
      "  Std Dev:  35.5878\n",
      "  Min (0th):   30.2995\n",
      "  25th:     153.6755\n",
      "  Median:   172.8446\n",
      "  75th:     196.0735\n",
      "  Max (100th):  285.3114\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 2,103 points\n",
      "\n",
      "Variable: SoilMoi100_200cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     287.1544\n",
      "  Std Dev:  44.7035\n",
      "  Min (0th):   56.4324\n",
      "  25th:     260.5154\n",
      "  Median:   287.0990\n",
      "  75th:     318.6728\n",
      "  Max (100th):  450.7496\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 1,416 points\n",
      "\n",
      "Exporting data to ./gldas_data_2013-01-01_2023-12-31.nc...\n",
      "Export complete!\n",
      "\n",
      "Data exported to: ./gldas_data_2013-01-01_2023-12-31.nc\n"
     ]
    }
   ],
   "source": [
    "# GLDAS example\n",
    "gldas_data = get_gldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving FLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for FLDAS granules (attempt 1/3)...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 13187.75it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [33:46<00:00, 15.35s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 349084.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/132 [00:00<00:22,  5.83file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure: FLDAS_NOAH01_C_GL_M.A201301.001.nc\n",
      "Dimensions: ['time', 'bnds', 'X', 'Y']\n",
      "Available variables: ['time_bnds', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'Psurf_f_tavg', 'Qair_f_tavg', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_tavg', 'Qsb_tavg', 'RadT_tavg', 'Rainf_f_tavg', 'SWE_inst', 'SWdown_f_tavg', 'SnowCover_inst', 'SnowDepth_inst', 'Snowf_tavg', 'Swnet_tavg', 'Tair_f_tavg', 'Wind_f_tavg', 'SoilMoi00_10cm_tavg', 'SoilMoi10_40cm_tavg', 'SoilMoi40_100cm_tavg', 'SoilMoi100_200cm_tavg', 'SoilTemp00_10cm_tavg', 'SoilTemp10_40cm_tavg', 'SoilTemp40_100cm_tavg', 'SoilTemp100_200cm_tavg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:01<00:00, 70.00file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Time resolution: 744 hours\n",
      "Number of timesteps: 132\n",
      "Spatial coverage: 45x79 grid points\n",
      "Y range: 40.550 to 44.950\n",
      "X range: -79.750 to -71.950\n",
      "\n",
      "Soil Moisture Statistics by Layer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilMoi00_10cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3622\n",
      "  Std Dev:  0.0497\n",
      "  Min (0th):   0.1411\n",
      "  25th:     0.3361\n",
      "  Median:   0.3690\n",
      "  75th:     0.3959\n",
      "  Max (100th):  0.4678\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,172 points\n",
      "\n",
      "Variable: SoilMoi10_40cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3800\n",
      "  Std Dev:  0.0672\n",
      "  Min (0th):   0.0897\n",
      "  25th:     0.3452\n",
      "  Median:   0.3867\n",
      "  75th:     0.4292\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,134 points\n",
      "\n",
      "Variable: SoilMoi40_100cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3456\n",
      "  Std Dev:  0.0804\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3084\n",
      "  Median:   0.3695\n",
      "  75th:     0.4006\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 23,405 points\n",
      "\n",
      "Variable: SoilMoi100_200cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3592\n",
      "  Std Dev:  0.0597\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3327\n",
      "  Median:   0.3670\n",
      "  75th:     0.3973\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 20,378 points\n",
      "\n",
      "Exporting data to ./fldas_data_2013-01-01_2023-12-31.nc...\n",
      "Export complete!\n",
      "\n",
      "Data exported to: ./fldas_data_2013-01-01_2023-12-31.nc\n"
     ]
    }
   ],
   "source": [
    "# FLDAS example\n",
    "fldas_data = get_fldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving SMAP soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "Found 10 granules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 10/10 [00:00<00:00, 1405.27it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 10/10 [03:30<00:00, 21.01s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 10/10 [00:00<00:00, 91779.08it/s]\n",
      "Processing:  10%|█         | 1/10 [00:00<00:01,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure:\n",
      "EASE2_global_projection:\n",
      "  Shape: (1,)\n",
      "  Dtype: |S1\n",
      "Geophysical_Data/baseflow_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/depth_to_water_table_from_surface_in_peat:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/free_surface_water_on_peat_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_ground:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_latent:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_sensible:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/height_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_evapotranspiration_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_saturated:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_snow_covered:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_unsaturated:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_wilting:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/leaf_area_index:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/mwrtm_vegopacity:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/net_downward_longwave_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/net_downward_shortwave_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/overland_runoff_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/precipitation_total_surface_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/radiation_longwave_absorbed_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/radiation_shortwave_downward_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile_pctl:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone_pctl:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_surface:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_surface_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_depth:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_mass:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_melt_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snowfall_surface_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer1:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer2:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer3:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer4:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer5:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer6:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_water_infiltration_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/specific_humidity_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/surface_pressure:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/surface_temp:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/temp_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/vegetation_greenness_fraction:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/windspeed_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_column:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: uint32\n",
      "  Fill Value: 4294967294\n",
      "cell_lat:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_lon:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_row:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: uint32\n",
      "  Fill Value: 4294967294\n",
      "time:\n",
      "  Shape: (1,)\n",
      "  Dtype: float64\n",
      "x:\n",
      "  Shape: (3856,)\n",
      "  Dtype: float64\n",
      "y:\n",
      "  Shape: (1624,)\n",
      "  Dtype: float64\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2136\n",
      "  Std Dev:  0.1390\n",
      "  Min:      0.0080\n",
      "  Max:      0.8854\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0063\n",
      "  Max:      0.9311\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 2/10 [00:00<00:01,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2135\n",
      "  Std Dev:  0.1389\n",
      "  Min:      0.0043\n",
      "  Max:      0.8869\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0064\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2135\n",
      "  Std Dev:  0.1389\n",
      "  Min:      0.0039\n",
      "  Max:      0.8894\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 3/10 [00:00<00:01,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std Dev:  0.1674\n",
      "  Min:      0.0064\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9295\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2131\n",
      "  Std Dev:  0.1391\n",
      "  Min:      0.0027\n",
      "  Max:      0.9020\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0065\n",
      "  Max:      0.9308\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 5/10 [00:00<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2125\n",
      "  Std Dev:  0.1393\n",
      "  Min:      0.0035\n",
      "  Max:      0.8945\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0063\n",
      "  Max:      0.9305\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 6/10 [00:00<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std Dev:  0.1392\n",
      "  Min:      0.0067\n",
      "  Max:      0.8925\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2414\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2122\n",
      "  Std Dev:  0.1393\n",
      "  Min:      0.0093\n",
      "  Max:      0.8780\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2414\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9300\n",
      "  Valid Points:    1,684,725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 8/10 [00:01<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2125\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0094\n",
      "  Max:      0.8894\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0064\n",
      "  Max:      0.9309\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 9/10 [00:01<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2129\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0088\n",
      "  Max:      0.8882\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0065\n",
      "  Max:      0.9308\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2130\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0062\n",
      "  Max:      0.8893\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:01<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0066\n",
      "  Max:      0.9306\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Retrieved data summary:\n",
      "Time period: 2013-01-01T00:00:00.000000000 to 2013-01-01T00:00:00.000000000\n",
      "Variables: ['sm_surface', 'sm_rootzone', 'sm_profile']\n",
      "\n",
      "Final Dataset Statistics:\n",
      "\n",
      "sm_surface:\n",
      "  Mean:     0.2129\n",
      "  Std Dev:  0.1392\n",
      "  Min:      0.0027\n",
      "  Max:      0.9020\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n",
      "\n",
      "sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9311\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n",
      "\n",
      "sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n"
     ]
    }
   ],
   "source": [
    "# SMAP example\n",
    "smap_data = get_smap_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving MERRA-2 monthly soil moisture data...\n",
      "Time range: 2013-01 to 2023-12\n",
      "Spatial bounds: lat [40.5, 45.02], lon [-79.77, -71.85]\n",
      "Variables: ['SFMC']\n",
      "\n",
      "Searching for data files...\n",
      "Found 132 files\n",
      "\n",
      "Downloading files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 349.03it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [06:49<00:00,  3.10s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 227557.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:06<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:30:00.000000000 to 2023-12-01T00:30:00.000000000\n",
      "Number of months: 132\n",
      "Spatial coverage: 10x13 grid points\n",
      "Lat range: 40.500 to 45.000\n",
      "Lon range: -79.375 to -71.875\n",
      "\n",
      "Variable Statistics:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SFMC\n",
      "Description: Surface Soil Moisture Content (0-5 cm)\n",
      "Statistics:\n",
      "  Mean:     0.2624\n",
      "  Std Dev:  0.0585\n",
      "  Min:      0.0841\n",
      "  25th:     0.2239\n",
      "  Median:   0.2646\n",
      "  75th:     0.3051\n",
      "  Max:      0.4069\n",
      "  Valid Points:    17,160\n",
      "  Missing Points:  0\n",
      "  Data Coverage:   100.0%\n",
      "\n",
      "Exporting data to merra2_data/merra2_monthly_soil_moisture_2013-01_2023-12.nc\n",
      "Export complete!\n",
      "Data retrieval and export successful!\n"
     ]
    }
   ],
   "source": [
    "# MERRA-2 data \n",
    "try:\n",
    "    ds = get_merra2_monthly_soil_moisture(\n",
    "        start_date=start_M,\n",
    "        end_date=end_M,\n",
    "        lat_bounds=lat_bounds,\n",
    "        lon_bounds=lon_bounds,\n",
    "        output_dir='merra2_data',\n",
    "        var_names=['SFMC']  # Optional: specify variables\n",
    "    )\n",
    "    if ds is not None:\n",
    "        print(\"Data retrieval and export successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve or export data: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
