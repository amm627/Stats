{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Moisture Data Sources\n",
    "\n",
    "This notebook implements standardized retrievers for multiple soil moisture data sources. Each source has unique characteristics:\n",
    "\n",
    "## Data Source Characteristics\n",
    "\n",
    "### Data Assimilation \n",
    "\n",
    "### ERA5\n",
    "- **Provider**: ECMWF (European Centre for Medium-Range Weather Forecasts)\n",
    "- **Resolution**: 0.1° x 0.1° (approximately 9km)\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Monthly updates, with 2-3 month delay\n",
    "- **Key Features**: High spatial resolution, consistent reanalysis\n",
    "\n",
    "### GLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.25° x 0.25°\n",
    "- **Temporal Coverage**: 2000-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Global coverage, multiple soil layers\n",
    "\n",
    "### NLDAS\n",
    "- **Provider**: NASA/NOAA\n",
    "- **Resolution**: 0.125° x 0.125°\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Hourly\n",
    "- **Key Features**: North American focus, high temporal resolution\n",
    "\n",
    "### FLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.1° x 0.1°\n",
    "- **Temporal Coverage**: 1982-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Africa-focused land data assimilation\n",
    "\n",
    "### MERRA-2\n",
    "- **Provider**: NASA GMAO\n",
    "- **Resolution**: 0.5° x 0.625°\n",
    "- **Temporal Coverage**: 1980-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Comprehensive atmospheric reanalysis\n",
    "\n",
    "### Remote Sensing \n",
    "\n",
    "### SMAP\n",
    "- **Provider**: NASA\n",
    "- **Resolution**: 9km x 9km\n",
    "- **Temporal Coverage**: 2015-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Direct satellite observations, high accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cdsapi\n",
    "import netCDF4\n",
    "import earthaccess\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import requests\n",
    "from time import sleep\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "from typing import Tuple, Optional,List, Dict\n",
    "from pathlib import Path\n",
    "import ftplib\n",
    "import ssl\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate statistics for the datasets \n",
    "def calculate_statistics(data, var_name, var_attrs):\n",
    "    \"\"\"\n",
    "    Calculate statistics for a variable based on its data type\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        The data array to analyze\n",
    "    var_name : str\n",
    "        Name of the variable\n",
    "    var_attrs : dict\n",
    "        Variable attributes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the statistics\n",
    "    \"\"\"\n",
    "    # Check if data is datetime type\n",
    "    if np.issubdtype(data.dtype, np.datetime64):\n",
    "        return {\n",
    "            'type': 'datetime',\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    # For numeric data\n",
    "    valid_data = data[~np.isnan(data)]\n",
    "    if len(valid_data) > 0:\n",
    "        return {\n",
    "            'type': 'numeric',\n",
    "            'mean': np.nanmean(data),\n",
    "            'median': np.nanmedian(data),\n",
    "            'std': np.nanstd(data),\n",
    "            'var': np.nanvar(data),\n",
    "            'min': np.nanmin(data),\n",
    "            'max': np.nanmax(data),\n",
    "            'valid_points': len(valid_data),\n",
    "            'missing_points': np.sum(np.isnan(data)),\n",
    "            'coverage': (len(valid_data) / data.size * 100),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'type': 'empty',\n",
    "        'shape': data.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 data retrieval with progress tracking\n",
    "def get_era5_data(dataset, request, output_file):\n",
    "    \"\"\"\n",
    "    Retrieve ERA5 data with progress tracking using a custom download approach\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : str\n",
    "        Name of the ERA5 dataset\n",
    "    request : dict\n",
    "        Request parameters for ERA5 data\n",
    "    output_file : str\n",
    "        Path to save the downloaded file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        The loaded dataset with ERA5 data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving ERA5 data...\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Time range: {request['year']}-{request['month']}\")\n",
    "    print(f\"Spatial bounds: {request['area']}\")\n",
    "    \n",
    "    try:\n",
    "        client = cdsapi.Client()\n",
    "        \n",
    "        # First, submit the request and get the result\n",
    "        print(\"Submitting request to ERA5...\")\n",
    "        result = client.retrieve(dataset, request)\n",
    "        \n",
    "        # Download the file with manual progress tracking\n",
    "        print(\"\\nDownloading data...\")\n",
    "        with open(output_file, 'wb') as f:\n",
    "            result.download(output_file)\n",
    "        \n",
    "        # Get file size after download\n",
    "        file_size = os.path.getsize(output_file)\n",
    "        print(f\"Download complete. File size: {file_size/1024/1024:.2f} MB\")\n",
    "        \n",
    "        # Load and analyze the downloaded data\n",
    "        print(\"\\nLoading dataset...\")\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        \n",
    "        # Print dataset information\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "        print(\"\\nVariables:\")\n",
    "        for var in ds.data_vars:\n",
    "            print(f\"\\nVariable: {var}\")\n",
    "            data = ds[var].values\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "            \n",
    "            # Get variable attributes\n",
    "            attrs = ds[var].attrs\n",
    "            units = attrs.get('units', 'unknown')\n",
    "            long_name = attrs.get('long_name', var)\n",
    "            \n",
    "            print(f\"Description: {long_name}\")\n",
    "            print(f\"Units: {units}\")\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if len(valid_data) > 0:\n",
    "                print(\"Statistics:\")\n",
    "                print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                print(f\"  Variance: {np.nanvar(data):.4f}\")\n",
    "                print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            else:\n",
    "                print(\"No valid data points found\")\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving ERA5 data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLDAS data request \n",
    "def get_nldas_data(start_date, end_date, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Retrieve NLDAS soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with NLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving NLDAS soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        # Authenticate with NASA Earthdata\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        # Search for granules\n",
    "        print(\"\\nSearching for NLDAS granules...\")\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"NLDAS_NOAH0125_H\",\n",
    "            version=\"2.0\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No NLDAS granules found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            \n",
    "            # Use tqdm for processing progress\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Select only soil moisture variables (SoilM_*)\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilM_' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting\n",
    "                    if 'lat' in ds.dims:\n",
    "                        ds = ds.sel(lat=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    if 'lon' in ds.dims:\n",
    "                        ds = ds.sel(lon=slice(lon_bounds[0], lon_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            # Print dataset information\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            # Print statistics for soil moisture variables\n",
    "            print(\"\\nSoil Moisture Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable attributes\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                # Calculate statistics for numeric data\n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving NLDAS soil moisture data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDAS data retrieval with progress tracking\n",
    "def get_gldas_data(start_date, end_date, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Retrieve GLDAS soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with GLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving GLDAS soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        # Search for granules\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"GLDAS_NOAH025_3H\",\n",
    "            version=\"2.1\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No GLDAS granules found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any granules\")\n",
    "                \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print dimensions and variables for first file\n",
    "                    if len(datasets) == 0:\n",
    "                        print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Data variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Select soil moisture variables (SoilMoi*)\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting\n",
    "                    if 'lat' in ds.dims:\n",
    "                        ds = ds.sel(lat=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    if 'lon' in ds.dims:\n",
    "                        ds = ds.sel(lon=slice(lon_bounds[0], lon_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            # Print dataset information\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            # Print statistics for soil moisture variables\n",
    "            print(\"\\nSoil Moisture Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable attributes\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                # Calculate statistics\n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving GLDAS soil moisture data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLDAS data retrieval with progress tracking\n",
    "def get_fldas_data(start_date, end_date, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Retrieve FLDAS soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with FLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving FLDAS soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"FLDAS_NOAH01_C_GL_M\",\n",
    "            version=\"001\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No FLDAS granules found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any granules\")\n",
    "                \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print dimensions and variables for first file\n",
    "                    if len(datasets) == 0:\n",
    "                        print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Data variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Select soil moisture variables (SoilMoi*cm_tavg)\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var and 'cm_tavg' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting using X/Y coordinates\n",
    "                    if 'X' in ds.dims and 'Y' in ds.dims:\n",
    "                        # Convert lat/lon bounds to X/Y if needed\n",
    "                        ds = ds.sel(X=slice(lon_bounds[0], lon_bounds[1]),\n",
    "                                  Y=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            print(\"\\nCombining datasets...\")\n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            print(\"\\nSoil Moisture Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving FLDAS soil moisture data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAP data retrieval with progress tracking\n",
    "def get_smap_data(start_date, end_date, lat_bounds, lon_bounds, max_files=10):\n",
    "    \"\"\"\n",
    "    Retrieve SMAP L4 soil moisture data with missing value handling\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving SMAP soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        granules = earthaccess.search_data(\n",
    "            count=max_files,\n",
    "            short_name=\"SPL4SMGP\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No SMAP granules found\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download granules\")\n",
    "                \n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing\"):\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as f:\n",
    "                        if len(datasets) == 0:\n",
    "                            print(\"\\nFile structure:\")\n",
    "                            def print_structure(name, obj):\n",
    "                                if isinstance(obj, h5py.Dataset):\n",
    "                                    print(f\"{name}:\")\n",
    "                                    print(f\"  Shape: {obj.shape}\")\n",
    "                                    print(f\"  Dtype: {obj.dtype}\")\n",
    "                                    if '_FillValue' in obj.attrs:\n",
    "                                        print(f\"  Fill Value: {obj.attrs['_FillValue']}\")\n",
    "                            f.visititems(print_structure)\n",
    "                        \n",
    "                        if 'Geophysical_Data' in f:\n",
    "                            geo_data = f['Geophysical_Data']\n",
    "                            sm_vars = ['sm_surface', 'sm_rootzone', 'sm_profile']\n",
    "                            ds_dict = {}\n",
    "                            \n",
    "                            time_value = None\n",
    "                            for attr in f.attrs.keys():\n",
    "                                if 'time' in attr.lower():\n",
    "                                    try:\n",
    "                                        time_str = f.attrs[attr]\n",
    "                                        if isinstance(time_str, bytes):\n",
    "                                            time_str = time_str.decode('utf-8')\n",
    "                                        time_value = pd.to_datetime(time_str)\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        continue\n",
    "                            \n",
    "                            if time_value is None:\n",
    "                                time_value = pd.Timestamp(start_date)\n",
    "                            \n",
    "                            for var in sm_vars:\n",
    "                                if var in geo_data:\n",
    "                                    # Get the data and attributes\n",
    "                                    data = geo_data[var][:]\n",
    "                                    attrs = dict(geo_data[var].attrs)\n",
    "                                    \n",
    "                                    # Handle missing values\n",
    "                                    # Check for _FillValue in attributes\n",
    "                                    fill_value = attrs.get('_FillValue', -9999)\n",
    "                                    # Replace both -9999 and the fill_value with NaN\n",
    "                                    data = np.where(data == -9999, np.nan, data)\n",
    "                                    if fill_value != -9999:\n",
    "                                        data = np.where(data == fill_value, np.nan, data)\n",
    "                                    \n",
    "                                    y_size, x_size = data.shape\n",
    "                                    coords = {\n",
    "                                        'y': np.linspace(lat_bounds[0], lat_bounds[1], y_size),\n",
    "                                        'x': np.linspace(lon_bounds[0], lon_bounds[1], x_size),\n",
    "                                        'time': [time_value]\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Print statistics for this variable\n",
    "                                    print(f\"\\nStatistics for {var}:\")\n",
    "                                    valid_data = data[~np.isnan(data)]\n",
    "                                    if len(valid_data) > 0:\n",
    "                                        print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                                        print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                                        print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                                        print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                                        print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                                        print(f\"  Units:           {attrs.get('units', 'unknown')}\")\n",
    "                                    \n",
    "                                    da = xr.DataArray(\n",
    "                                        data[np.newaxis, :, :],\n",
    "                                        dims=['time', 'y', 'x'],\n",
    "                                        coords=coords,\n",
    "                                        name=var,\n",
    "                                        attrs=attrs\n",
    "                                    )\n",
    "                                    ds_dict[var] = da\n",
    "                            \n",
    "                            if ds_dict:\n",
    "                                ds = xr.Dataset(ds_dict)\n",
    "                                datasets.append(ds)\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process {os.path.basename(file_path)}: {str(e)}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found\")\n",
    "            \n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            print(\"\\nRetrieved data summary:\")\n",
    "            print(f\"Time period: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(\"Variables:\", list(combined_ds.data_vars))\n",
    "            \n",
    "            # Print final statistics for combined dataset\n",
    "            print(\"\\nFinal Dataset Statistics:\")\n",
    "            for var in combined_ds.data_vars:\n",
    "                data = combined_ds[var].values\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                print(f\"\\n{var}:\")\n",
    "                if len(valid_data) > 0:\n",
    "                    print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                    print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                    print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                    print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                    print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                    print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                    print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERRA-2 data retrieval with progress tracking\n",
    "# Cell 1: Function Definition\n",
    "\n",
    "def get_merra2_data(\n",
    "    bbox: Tuple[float, float, float, float],\n",
    "    date_range: Tuple[str, str],\n",
    "    var_names: Optional[List[str]] = None,\n",
    "    output_file: str = 'merra_2_soil_moisture_data.nc',\n",
    "    product: str = 'M2T1NXLND_5.12.4'\n",
    ") -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Fetch MERRA-2 data and perform basic statistical analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bbox : tuple\n",
    "        Bounding box coordinates as (min_lon, min_lat, max_lon, max_lat)\n",
    "        Valid ranges: longitude [-180, 180], latitude [-90, 90]\n",
    "    \n",
    "    date_range : tuple\n",
    "        Start and end dates as ('YYYY-MM-DD', 'YYYY-MM-DD')\n",
    "    \n",
    "    var_names : list, optional\n",
    "        List of variable names to fetch. If None, defaults to \n",
    "        ['SFMC', 'GWETTOP', 'PRMC', 'RZMC']\n",
    "    \n",
    "    output_file : str, optional\n",
    "        Output filename for NetCDF data\n",
    "        Default: 'merra_2_soil_moisture_data.nc'\n",
    "    \n",
    "    product : str, optional\n",
    "        MERRA-2 product identifier\n",
    "        Default: 'M2T1NXLND_5.12.4'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset or None\n",
    "        Dataset containing the requested variables with statistics printed\n",
    "        Returns None if the request fails\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> # Fetch data for New York State\n",
    "    >>> ds = fetch_merra2_data(\n",
    "    ...     bbox=(-79.77, 40.5, -71.85, 45.02),\n",
    "    ...     date_range=('2020-01-01', '2020-01-31'),\n",
    "    ...     var_names=['SFMC', 'PRMC']\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    # Parameter validation\n",
    "    try:\n",
    "        # Validate and process bbox\n",
    "        minlon, minlat, maxlon, maxlat = bbox\n",
    "        if not (-180 <= minlon <= 180 and -180 <= maxlon <= 180):\n",
    "            raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "        if not (-90 <= minlat <= 90 and -90 <= maxlat <= 90):\n",
    "            raise ValueError(\"Latitude must be between -90 and 90 degrees\")\n",
    "        if minlon >= maxlon or minlat >= maxlat:\n",
    "            raise ValueError(\"Min values must be less than max values\")\n",
    "\n",
    "        # Validate and process dates\n",
    "        start_date, end_date = date_range\n",
    "        try:\n",
    "            datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Dates must be in YYYY-MM-DD format\")\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"Start date must be before end date\")\n",
    "\n",
    "        # Set default variables if none provided\n",
    "        if var_names is None:\n",
    "            var_names = ['SFMC', 'GWETTOP', 'PRMC', 'RZMC']\n",
    "            print(f\"Using default variables: {var_names}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Parameter validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize urllib PoolManager and set base URL\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    url = 'https://disc.gsfc.nasa.gov/service/subset/jsonwsp'\n",
    "    \n",
    "    def get_http_data(request):\n",
    "        hdrs = {'Content-Type': 'application/json',\n",
    "                'Accept': 'application/json'}\n",
    "        data = json.dumps(request)       \n",
    "        r = http.request('POST', url, body=data, headers=hdrs)\n",
    "        response = json.loads(r.data)   \n",
    "        if response['type'] == 'jsonwsp/fault':\n",
    "            print('API Error: faulty %s request' % response['methodname'])\n",
    "            sys.exit(1)\n",
    "        return response\n",
    "    \n",
    "    # Construct the subset request\n",
    "    subset_request = {\n",
    "        'methodname': 'subset',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'version': '1.0',\n",
    "        'args': {\n",
    "            'role': 'subset',\n",
    "            'start': start_date,\n",
    "            'end': end_date,\n",
    "            'box': [minlon, minlat, maxlon, maxlat],\n",
    "            'crop': True,\n",
    "            'data': [{'datasetId': product,\n",
    "                      'variable': varName\n",
    "                     } for varName in var_names]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Submit request and get job ID\n",
    "    response = get_http_data(subset_request)\n",
    "    myJobId = response['result']['jobId']\n",
    "    print('Job ID:', myJobId)\n",
    "    print('Initial status:', response['result']['Status'])\n",
    "    \n",
    "    # Monitor job status\n",
    "    status_request = {\n",
    "        'methodname': 'GetStatus',\n",
    "        'version': '1.0',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'args': {'jobId': myJobId}\n",
    "    }\n",
    "    \n",
    "    while response['result']['Status'] in ['Accepted', 'Running']:\n",
    "        time.sleep(5)\n",
    "        response = get_http_data(status_request)\n",
    "        status = response['result']['Status']\n",
    "        percent = response['result']['PercentCompleted']\n",
    "        print(f'Job status: {status} ({percent}% complete)')\n",
    "    \n",
    "    def download_netcdf(url, output_file):\n",
    "        print(f\"\\nDownloading data to {output_file}...\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(output_file, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully saved data to {output_file}\")\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Get results and download data\n",
    "    if response['result']['Status'] == 'Succeeded':\n",
    "        print('Job Finished:', response['result']['message'])\n",
    "        \n",
    "        result = requests.get('https://disc.gsfc.nasa.gov/api/jobs/results/'+myJobId)\n",
    "        try:\n",
    "            result.raise_for_status()\n",
    "            urls = result.text.split('\\n')\n",
    "            \n",
    "            success = False\n",
    "            for url in urls:\n",
    "                if url.strip():\n",
    "                    print(\"\\nAttempting download from:\", url)\n",
    "                    success = download_netcdf(url, output_file)\n",
    "                    if success:\n",
    "                        break\n",
    "            \n",
    "            if success:\n",
    "                # Load and analyze the data\n",
    "                ds = xr.open_dataset(output_file)\n",
    "                \n",
    "                print(\"\\n=== Dataset Information ===\")\n",
    "                print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "                print(f\"\\nVariables: {list(ds.data_vars)}\")\n",
    "                print(f\"\\nTime range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "                print(f\"Spatial extent: {ds.lon.values.min():.2f}°E to {ds.lon.values.max():.2f}°E, \"\n",
    "                      f\"{ds.lat.values.min():.2f}°N to {ds.lat.values.max():.2f}°N\")\n",
    "                \n",
    "                for var in ds.data_vars:\n",
    "                    print(f\"\\n=== Statistics for {var} ===\")\n",
    "                    data = ds[var]\n",
    "                    print(f\"Shape: {data.shape}\")\n",
    "                    print(f\"Missing values: {data.isnull().sum().values}\")\n",
    "                    print(f\"Mean: {float(data.mean()):.4f}\")\n",
    "                    print(f\"Min: {float(data.min()):.4f}\")\n",
    "                    print(f\"Max: {float(data.max()):.4f}\")\n",
    "                    print(f\"Standard deviation: {float(data.std()):.4f}\")\n",
    "                \n",
    "                return ds\n",
    "            else:\n",
    "                print(\"Failed to download data. Please check your credentials and try again.\")\n",
    "                return None\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Error getting download URLs:', e)\n",
    "            return None\n",
    "    else:\n",
    "        print('Job Failed:', response['fault']['code'])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smos data request\n",
    "\n",
    "def get_file_size(ftps: ftplib.FTP_TLS, filename: str) -> int:\n",
    "    \"\"\"Get file size in bytes\"\"\"\n",
    "    try:\n",
    "        return ftps.size(filename)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def download_with_progress(ftps: ftplib.FTP_TLS, filename: str, local_path: str) -> bool:\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    file_size = get_file_size(ftps, filename)\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=file_size, unit='B', unit_scale=True, \n",
    "                desc=f\"Downloading {os.path.basename(filename)}\")\n",
    "    \n",
    "    # Create callback for updating progress\n",
    "    def callback(data):\n",
    "        pbar.update(len(data))\n",
    "        with open(local_path, 'ab') as f:\n",
    "            f.write(data)\n",
    "    \n",
    "    try:\n",
    "        # Clear existing file if any\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "            \n",
    "        # Download file\n",
    "        ftps.retrbinary(f'RETR {filename}', callback)\n",
    "        pbar.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        pbar.close()\n",
    "        print(f\"Error downloading {filename}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_smos_data(start_date, end_date, lat_bounds, lon_bounds, username=None, password=None):\n",
    "    \"\"\"\n",
    "    Retrieve SMOS soil moisture data using FTPS from ESA's dissemination service with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for region of interest\n",
    "    username : str, optional\n",
    "        ESA data portal username\n",
    "    password : str, optional\n",
    "        ESA data portal password\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with SMOS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing SMOS FTPS connection...\")\n",
    "    \n",
    "    # Get credentials if not provided\n",
    "    if not username or not password:\n",
    "        print(\"\\nESA credentials required for SMOS data access.\")\n",
    "        print(\"Please use your EO-Sign In account credentials.\")\n",
    "        username = input(\"ESA username: \") if not username else username\n",
    "        password = getpass.getpass(\"ESA password: \") if not password else password\n",
    "\n",
    "    try:\n",
    "        # Create SSL context\n",
    "        ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
    "        ssl_context.check_hostname = False\n",
    "        ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "        # Connect to SMOS FTPS server\n",
    "        ftps = ftplib.FTP_TLS()\n",
    "        ftps.ssl_version = ssl.PROTOCOL_TLS\n",
    "        ftps.context = ssl_context\n",
    "        \n",
    "        # Connect with progress updates\n",
    "        print(\"Connecting to SMOS FTPS server...\")\n",
    "        ftps.connect('smos-diss.eo.esa.int', 990)\n",
    "        ftps.login(username, password)\n",
    "        ftps.prot_p()\n",
    "        \n",
    "        print(\"Successfully connected to SMOS FTPS server\")\n",
    "        \n",
    "        # Convert dates\n",
    "        start_dt = pd.to_datetime(start_date)\n",
    "        end_dt = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Navigate to SMOS data directory\n",
    "        base_path = '/SMOS/L2SM'\n",
    "        ftps.cwd(base_path)\n",
    "        \n",
    "        # Collect all matching files first\n",
    "        matching_files: List[Dict] = []\n",
    "        \n",
    "        # Calculate total directories to search\n",
    "        years = range(start_dt.year, end_dt.year + 1)\n",
    "        total_months = sum(12 if year != end_dt.year else end_dt.month \n",
    "                         for year in years)\n",
    "        \n",
    "        # Create progress bar for directory scanning\n",
    "        with tqdm(total=total_months, desc=\"Scanning directories\") as dir_pbar:\n",
    "            for year in years:\n",
    "                year_path = f\"{year:04d}\"\n",
    "                try:\n",
    "                    ftps.cwd(f\"{base_path}/{year_path}\")\n",
    "                    \n",
    "                    # Determine month range for current year\n",
    "                    start_month = 1 if year != start_dt.year else start_dt.month\n",
    "                    end_month = 12 if year != end_dt.year else end_dt.month\n",
    "                    \n",
    "                    for month in range(start_month, end_month + 1):\n",
    "                        month_path = f\"{month:02d}\"\n",
    "                        try:\n",
    "                            ftps.cwd(f\"{base_path}/{year_path}/{month_path}\")\n",
    "                            \n",
    "                            # List files in directory\n",
    "                            files = []\n",
    "                            ftps.dir(files.append)\n",
    "                            \n",
    "                            # Filter files by date and type\n",
    "                            for file_info in files:\n",
    "                                if not file_info.startswith('d'):\n",
    "                                    filename = file_info.split()[-1]\n",
    "                                    if filename.endswith('.nc') or filename.endswith('.DBL'):\n",
    "                                        try:\n",
    "                                            file_date = pd.to_datetime(filename.split('_')[4][:8])\n",
    "                                            if start_dt <= file_date <= end_dt:\n",
    "                                                file_path = f\"{base_path}/{year_path}/{month_path}/{filename}\"\n",
    "                                                matching_files.append({\n",
    "                                                    'path': file_path,\n",
    "                                                    'name': filename,\n",
    "                                                    'date': file_date,\n",
    "                                                    'size': get_file_size(ftps, filename)\n",
    "                                                })\n",
    "                                        except Exception:\n",
    "                                            continue\n",
    "                                            \n",
    "                        except ftplib.error_perm:\n",
    "                            print(f\"Could not access month directory: {month_path}\")\n",
    "                        \n",
    "                        dir_pbar.update(1)\n",
    "                            \n",
    "                except ftplib.error_perm:\n",
    "                    print(f\"Could not access year directory: {year_path}\")\n",
    "        \n",
    "        if not matching_files:\n",
    "            raise ValueError(\"No SMOS files found for the specified date range\")\n",
    "        \n",
    "        print(f\"\\nFound {len(matching_files)} matching files\")\n",
    "        \n",
    "        # Download and process files\n",
    "        datasets = []\n",
    "        total_size = sum(f['size'] for f in matching_files)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create main progress bar for overall download progress\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True,\n",
    "                     desc=\"Overall download progress\") as main_pbar:\n",
    "                \n",
    "                for file_info in matching_files:\n",
    "                    local_file = os.path.join(temp_dir, file_info['name'])\n",
    "                    \n",
    "                    # Download file with its own progress bar\n",
    "                    if download_with_progress(ftps, file_info['path'], local_file):\n",
    "                        try:\n",
    "                            # Process file with progress update\n",
    "                            with tqdm(total=1, desc=f\"Processing {file_info['name']}\", \n",
    "                                    leave=False) as proc_pbar:\n",
    "                                ds = xr.open_dataset(local_file)\n",
    "                                \n",
    "                                # Select soil moisture variables\n",
    "                                sm_vars = [var for var in ds.data_vars \n",
    "                                         if any(kw in var.lower() \n",
    "                                               for kw in ['sm', 'soil_moisture', 'moisture'])]\n",
    "                                \n",
    "                                if sm_vars:\n",
    "                                    ds = ds[sm_vars]\n",
    "                                    datasets.append(ds)\n",
    "                                \n",
    "                                proc_pbar.update(1)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to process {file_info['name']}: {str(e)}\")\n",
    "                            continue\n",
    "                            \n",
    "                    main_pbar.update(file_info['size'])\n",
    "        \n",
    "        ftps.quit()\n",
    "        \n",
    "        if not datasets:\n",
    "            raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "        \n",
    "        # Combine datasets with progress bar\n",
    "        print(\"\\nMerging datasets...\")\n",
    "        with tqdm(total=1, desc=\"Merging datasets\") as merge_pbar:\n",
    "            combined_ds = xr.merge(datasets)\n",
    "            merge_pbar.update(1)\n",
    "        \n",
    "        # Print dataset information with formatted output\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "        \n",
    "        # Create progress bar for statistical analysis\n",
    "        with tqdm(total=len(combined_ds.data_vars), \n",
    "                 desc=\"Calculating statistics\") as stat_pbar:\n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                \n",
    "                if len(valid_data) > 0:\n",
    "                    print(\"Statistics:\")\n",
    "                    print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                    print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                    print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                    print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                    print(f\"  Shape:    {data.shape}\")\n",
    "                    print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                    print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                    print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                \n",
    "                stat_pbar.update(1)\n",
    "        \n",
    "        return combined_ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentinel-1 request\n",
    "def get_sentinel1_data(start_date, end_date, lat_bounds, lon_bounds, max_files=10):\n",
    "    \"\"\"\n",
    "    Retrieve Sentinel-1 soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for region of interest\n",
    "    max_files : int\n",
    "        Maximum number of files to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with Sentinel-1 data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving Sentinel-1 data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"SENTINEL-1A_SLC\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1]),\n",
    "            count=max_files\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No Sentinel-1 granules found\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download granules\")\n",
    "            \n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Select relevant variables (adjust based on actual variable names)\n",
    "                    vars_of_interest = [var for var in ds.data_vars \n",
    "                                     if any(keyword in var.lower() \n",
    "                                           for keyword in ['vv', 'vh', 'angle', 'sigma'])]\n",
    "                    if not vars_of_interest:\n",
    "                        continue\n",
    "                    ds = ds[vars_of_interest]\n",
    "                    \n",
    "                    # Apply spatial subsetting if coordinates exist\n",
    "                    if 'latitude' in ds.dims:\n",
    "                        ds = ds.sel(latitude=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    if 'longitude' in ds.dims:\n",
    "                        ds = ds.sel(longitude=slice(lon_bounds[0], lon_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid data found in downloaded files\")\n",
    "            \n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'unknown')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving Sentinel-1 data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "lat_bounds = (40.5, 45.02)\n",
    "lon_bounds = (-79.77, -71.85)\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5_land request:\n",
    "request = {\n",
    "    \"variable\": \"volumetric_soil_water_layer_1\",\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"year\": \"2023\",\n",
    "    \"month\": \"01\",\n",
    "    \"day\": [\"01\", \"02\"],\n",
    "    \"time\": [f\"{hour:02d}:00\" for hour in range(24)],\n",
    "    \"area\": [45.02, -79.77, 40.5, -71.85],  # [north, west, south, east]\n",
    "    \"format\": \"netcdf\"\n",
    " }\n",
    "    \n",
    "    # Get the data\n",
    "try:\n",
    "     era5_data = get_era5_data(\n",
    "        dataset=\"reanalysis-era5-land\",\n",
    "        request=request,\n",
    "        output_file=\"era5_soil_moisture.nc\"\n",
    "    )  \n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve ERA5 data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage NLDAS:\n",
    "if __name__ == \"__main__\":\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2023-01-02\"\n",
    "    lat_bounds = (40.5, 45.02)\n",
    "    lon_bounds = (-79.77, -71.85)\n",
    "    \n",
    "    try:\n",
    "        # Set up output file name\n",
    "        output_file = f\"nldas_data_{start_date}_{end_date}.nc\"\n",
    "        \n",
    "        # Retrieve the data\n",
    "        nldas_data = get_nldas_data(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            lat_bounds=lat_bounds,\n",
    "            lon_bounds=lon_bounds\n",
    "        )\n",
    "        \n",
    "        # Save the data\n",
    "        print(f\"\\nSaving data to {output_file}...\")\n",
    "        nldas_data.to_netcdf(output_file)\n",
    "        \n",
    "        # Print file size\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"File saved successfully. Size: {file_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve NLDAS data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDAS example\n",
    "gldas_data = get_gldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLDAS example\n",
    "fldas_data = get_fldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAP example\n",
    "smap_data = get_smap_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 67239b288be99c3db6e2b0ad\n",
      "Initial status: Accepted\n",
      "Job status: Succeeded (100% complete)\n",
      "Job Finished: Complete (M2T1NXLND_5.12.4)\n",
      "\n",
      "Attempting download from: https://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2T1NXLND.5.12.4/doc/MERRA2.README.pdf\n",
      "\n",
      "Downloading data to merra_2_soil_moisture_data.nc...\n",
      "Error downloading data: 404 Client Error: Not Found for url: https://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2T1NXLND.5.12.4/doc/MERRA2.README.pdf%0D\n",
      "\n",
      "Attempting download from: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXLND.5.12.4/2020/01/MERRA2_400.tavg1_2d_lnd_Nx.20200101.nc4.nc4?SFMC[0:23][261:270][160:173],PRMC[0:23][261:270][160:173],time,lat[261:270],lon[160:173]\n",
      "\n",
      "Downloading data to merra_2_soil_moisture_data.nc...\n",
      "Successfully saved data to merra_2_soil_moisture_data.nc\n",
      "\n",
      "=== Dataset Information ===\n",
      "Dimensions: {'time': 24, 'lat': 10, 'lon': 14}\n",
      "\n",
      "Variables: ['SFMC', 'PRMC']\n",
      "\n",
      "Time range: 2020-01-01T00:30:00.000000000 to 2020-01-01T23:30:00.000000000\n",
      "Spatial extent: -80.00°E to -71.88°E, 40.50°N to 45.00°N\n",
      "\n",
      "=== Statistics for SFMC ===\n",
      "Shape: (24, 10, 14)\n",
      "Missing values: 0\n",
      "Mean: 0.2950\n",
      "Min: 0.1709\n",
      "Max: 0.3823\n",
      "Standard deviation: 0.0403\n",
      "\n",
      "=== Statistics for PRMC ===\n",
      "Shape: (24, 10, 14)\n",
      "Missing values: 0\n",
      "Mean: 0.2829\n",
      "Min: 0.1565\n",
      "Max: 0.3816\n",
      "Standard deviation: 0.0463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/0tkgm2g11c9202ygt6n5r_qh0000gn/T/ipykernel_94552/3139272757.py:169: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# MERRA-2 data \n",
    "merra2_data = get_merra2_data(\n",
    "    bbox=(-79.77, 40.5, -71.85, 45.02),  # (min_lon, min_lat, max_lon, max_lat)\n",
    "    date_range=('2020-01-01', '2020-01-31'),\n",
    "    var_names=['SFMC', 'PRMC'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing SMOS FTPS connection...\n",
      "\n",
      "ESA credentials required for SMOS data access.\n",
      "Please use your EO-Sign In account credentials.\n",
      "Connecting to SMOS FTPS server...\n",
      "\n",
      "Error: \n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m smos_data\u001b[38;5;241m=\u001b[39m\u001b[43mget_smos_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon_bounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 96\u001b[0m, in \u001b[0;36mget_smos_data\u001b[0;34m(start_date, end_date, lat_bounds, lon_bounds, username, password)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Connect with progress updates\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnecting to SMOS FTPS server...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mftps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmos-diss.eo.esa.int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m990\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m ftps\u001b[38;5;241m.\u001b[39mlogin(username, password)\n\u001b[1;32m     98\u001b[0m ftps\u001b[38;5;241m.\u001b[39mprot_p()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/climate_env/lib/python3.10/ftplib.py:162\u001b[0m, in \u001b[0;36mFTP.connect\u001b[0;34m(self, host, port, timeout, source_address)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mfamily\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwelcome \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwelcome\n",
      "File \u001b[0;32m/opt/anaconda3/envs/climate_env/lib/python3.10/ftplib.py:244\u001b[0m, in \u001b[0;36mFTP.getresp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetresp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 244\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetmultiline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugging:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*resp*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize(resp))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/climate_env/lib/python3.10/ftplib.py:230\u001b[0m, in \u001b[0;36mFTP.getmultiline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetmultiline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 230\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    232\u001b[0m         code \u001b[38;5;241m=\u001b[39m line[:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/climate_env/lib/python3.10/ftplib.py:218\u001b[0m, in \u001b[0;36mFTP.getline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*get*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize(line))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m CRLF:\n\u001b[1;32m    220\u001b[0m     line \u001b[38;5;241m=\u001b[39m line[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smos_data=get_smos_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel1_data=get_sentinel1_data(start_date, end_date, lat_bounds, lon_bounds, max_files=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
