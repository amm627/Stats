{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Moisture Data Sources\n",
    "\n",
    "This notebook implements standardized retrievers for multiple soil moisture data sources. Each source has unique characteristics:\n",
    "\n",
    "## Data Source Characteristics\n",
    "\n",
    "### Data Assimilation \n",
    "\n",
    "### ERA5\n",
    "- **Provider**: ECMWF (European Centre for Medium-Range Weather Forecasts)\n",
    "- **Resolution**: 0.1° x 0.1° (approximately 9km)\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Monthly updates, with 2-3 month delay\n",
    "- **Key Features**: High spatial resolution, consistent reanalysis\n",
    "\n",
    "### GLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.25° x 0.25°\n",
    "- **Temporal Coverage**: 2000-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Global coverage, multiple soil layers\n",
    "\n",
    "### NLDAS\n",
    "- **Provider**: NASA/NOAA\n",
    "- **Resolution**: 0.125° x 0.125°\n",
    "- **Temporal Coverage**: 1979-present\n",
    "- **Update Frequency**: Hourly\n",
    "- **Key Features**: North American focus, high temporal resolution\n",
    "\n",
    "### FLDAS\n",
    "- **Provider**: NASA GSFC\n",
    "- **Resolution**: 0.1° x 0.1°\n",
    "- **Temporal Coverage**: 1982-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Africa-focused land data assimilation\n",
    "\n",
    "### MERRA-2\n",
    "- **Provider**: NASA GMAO\n",
    "- **Resolution**: 0.5° x 0.625°\n",
    "- **Temporal Coverage**: 1980-present\n",
    "- **Update Frequency**: Monthly\n",
    "- **Key Features**: Comprehensive atmospheric reanalysis\n",
    "\n",
    "### Remote Sensing \n",
    "\n",
    "### SMAP\n",
    "- **Provider**: NASA\n",
    "- **Resolution**: 9km x 9km\n",
    "- **Temporal Coverage**: 2015-present\n",
    "- **Update Frequency**: 3-hourly\n",
    "- **Key Features**: Direct satellite observations, high accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/climate_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import necessary packages \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cdsapi\n",
    "import netCDF4\n",
    "import earthaccess\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import requests\n",
    "from time import sleep\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "from typing import Tuple, Optional,List, Dict\n",
    "from pathlib import Path\n",
    "import ftplib\n",
    "import ssl\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "max_lat= 45.02\n",
    "min_lat= 40.5\n",
    "max_lon= -71.85\n",
    "min_lon= -79.77\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "lat_bounds = (min_lat, max_lat)\n",
    "lon_bounds = (min_lon, max_lon)\n",
    "area=(max_lat, min_lon, min_lat, max_lon) # (max_lat, min_lon, min_lat, max_lon)\n",
    "bbox=(min_lon, min_lat, max_lon, max_lat) # (min_lon, min_lat, max_lon, max_lat)\n",
    "date_range=(start_date, end_date) #start_dat ,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate statistics for the datasets \n",
    "def calculate_statistics(data, var_name, var_attrs):\n",
    "    \"\"\"\n",
    "    Calculate statistics for a variable based on its data type\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        The data array to analyze\n",
    "    var_name : str\n",
    "        Name of the variable\n",
    "    var_attrs : dict\n",
    "        Variable attributes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the statistics\n",
    "    \"\"\"\n",
    "    # Check if data is datetime type\n",
    "    if np.issubdtype(data.dtype, np.datetime64):\n",
    "        return {\n",
    "            'type': 'datetime',\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    # For numeric data\n",
    "    valid_data = data[~np.isnan(data)]\n",
    "    if len(valid_data) > 0:\n",
    "        return {\n",
    "            'type': 'numeric',\n",
    "            'mean': np.nanmean(data),\n",
    "            'median': np.nanmedian(data),\n",
    "            'std': np.nanstd(data),\n",
    "            'var': np.nanvar(data),\n",
    "            'min': np.nanmin(data),\n",
    "            'max': np.nanmax(data),\n",
    "            'valid_points': len(valid_data),\n",
    "            'missing_points': np.sum(np.isnan(data)),\n",
    "            'coverage': (len(valid_data) / data.size * 100),\n",
    "            'shape': data.shape\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'type': 'empty',\n",
    "        'shape': data.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 data retrieval with progress tracking\n",
    "def get_era5_data(dataset, request, output_file):\n",
    "    \"\"\"\n",
    "    Retrieve ERA5 data with progress tracking using a custom download approach\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : str\n",
    "        Name of the ERA5 dataset\n",
    "    request : dict\n",
    "        Request parameters for ERA5 data\n",
    "    output_file : str\n",
    "        Path to save the downloaded file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        The loaded dataset with ERA5 data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving ERA5 data...\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Time range: {request['year']}-{request['month']}\")\n",
    "    print(f\"Spatial bounds: {request['area']}\")\n",
    "    \n",
    "    try:\n",
    "        client = cdsapi.Client()\n",
    "        \n",
    "        # First, submit the request and get the result\n",
    "        print(\"Submitting request to ERA5...\")\n",
    "        result = client.retrieve(dataset, request)\n",
    "        \n",
    "        # Download the file with manual progress tracking\n",
    "        print(\"\\nDownloading data...\")\n",
    "        with open(output_file, 'wb') as f:\n",
    "            result.download(output_file)\n",
    "        \n",
    "        # Get file size after download\n",
    "        file_size = os.path.getsize(output_file)\n",
    "        print(f\"Download complete. File size: {file_size/1024/1024:.2f} MB\")\n",
    "        \n",
    "        # Load and analyze the downloaded data\n",
    "        print(\"\\nLoading dataset...\")\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        \n",
    "        # Print dataset information\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "        print(\"\\nVariables:\")\n",
    "        for var in ds.data_vars:\n",
    "            print(f\"\\nVariable: {var}\")\n",
    "            data = ds[var].values\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "            \n",
    "            # Get variable attributes\n",
    "            attrs = ds[var].attrs\n",
    "            units = attrs.get('units', 'unknown')\n",
    "            long_name = attrs.get('long_name', var)\n",
    "            \n",
    "            print(f\"Description: {long_name}\")\n",
    "            print(f\"Units: {units}\")\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if len(valid_data) > 0:\n",
    "                print(\"Statistics:\")\n",
    "                print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                print(f\"  Variance: {np.nanvar(data):.4f}\")\n",
    "                print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            else:\n",
    "                print(\"No valid data points found\")\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving ERA5 data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLDAS data request \n",
    "def get_nldas_data(start_date, end_date, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Retrieve NLDAS soil moisture data with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with NLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving NLDAS soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        # Authenticate with NASA Earthdata\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        # Search for granules\n",
    "        print(\"\\nSearching for NLDAS granules...\")\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name=\"NLDAS_NOAH0125_M\",\n",
    "            version=\"2.0\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No NLDAS granules found for the specified parameters\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            \n",
    "            # Use tqdm for processing progress\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Select only soil moisture variables (SoilM_*)\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilM_' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting\n",
    "                    if 'lat' in ds.dims:\n",
    "                        ds = ds.sel(lat=slice(lat_bounds[0], lat_bounds[1]))\n",
    "                    if 'lon' in ds.dims:\n",
    "                        ds = ds.sel(lon=slice(lon_bounds[0], lon_bounds[1]))\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process file {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            \n",
    "            # Print dataset information\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Dimensions: {dict(combined_ds.sizes)}\")\n",
    "            \n",
    "            # Print statistics for soil moisture variables\n",
    "            print(\"\\nSoil Moisture Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable attributes\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                # Calculate statistics for numeric data\n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Median:   {np.nanmedian(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min:      {np.nanmin(data):.4f}\")\n",
    "                        print(f\"  Max:      {np.nanmax(data):.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                    else:\n",
    "                        print(\"No valid numeric data found\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError retrieving NLDAS soil moisture data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDAS data retrieval with progress tracking\n",
    "def validate_dates(start_date: str, end_date: str) -> tuple:\n",
    "    \"\"\"Validate and parse date strings.\"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        if end < start:\n",
    "            raise ValueError(\"End date must be after start date\")\n",
    "        return start, end\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Please use YYYY-MM-DD format. Error: {str(e)}\")\n",
    "\n",
    "def validate_bounds(lat_bounds: tuple, lon_bounds: tuple) -> None:\n",
    "    \"\"\"Validate spatial bounds for GLDAS data.\"\"\"\n",
    "    if not isinstance(lat_bounds, tuple) or not isinstance(lon_bounds, tuple):\n",
    "        raise TypeError(\"Bounds must be tuples\")\n",
    "    if len(lat_bounds) != 2 or len(lon_bounds) != 2:\n",
    "        raise ValueError(\"Bounds must contain exactly 2 values\")\n",
    "    if not (-60 <= lat_bounds[0] <= 90 and -60 <= lat_bounds[1] <= 90):\n",
    "        raise ValueError(\"GLDAS latitude must be between -60 and 90 degrees\")\n",
    "    if not (-180 <= lon_bounds[0] <= 180 and -180 <= lon_bounds[1] <= 180):\n",
    "        raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "    if lat_bounds[0] >= lat_bounds[1]:\n",
    "        raise ValueError(\"Minimum latitude must be less than maximum latitude\")\n",
    "    if lon_bounds[0] >= lon_bounds[1]:\n",
    "        raise ValueError(\"Minimum longitude must be less than maximum longitude\")\n",
    "\n",
    "def export_dataset(ds: xr.Dataset, start_date: str, end_date: str, output_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export dataset to NetCDF file with date-specific filename.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset to export\n",
    "    start_date : str\n",
    "        Start date string\n",
    "    end_date : str\n",
    "        End date string\n",
    "    output_dir : str, optional\n",
    "        Directory to save the file (default: current directory)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the exported file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = '.'\n",
    "        \n",
    "    # Create filename\n",
    "    filename = f\"gldas_data_{start_date}_{end_date}.nc\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add export metadata\n",
    "    ds.attrs['export_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds.attrs['data_period'] = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    # Export to NetCDF\n",
    "    print(f\"\\nExporting data to {filepath}...\")\n",
    "    ds.to_netcdf(filepath)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def get_gldas_data(start_date: str, end_date: str, lat_bounds: tuple, lon_bounds: tuple, \n",
    "                   output_dir: str = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieve GLDAS soil moisture data with progress tracking and validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest (between -60 and 90)\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest (-180 to 180)\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output file (default: current directory)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with GLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_dates(start_date, end_date)\n",
    "        validate_bounds(lat_bounds, lon_bounds)\n",
    "        \n",
    "        print(\"\\nRetrieving GLDAS soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        try:\n",
    "            auth = earthaccess.login()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to authenticate with NASA Earthdata: {str(e)}\")\n",
    "        \n",
    "        # Search for granules with retry mechanism\n",
    "        max_retries = 3\n",
    "        granules = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nSearching for GLDAS granules (attempt {attempt + 1}/{max_retries})...\")\n",
    "                granules = earthaccess.search_data(\n",
    "                    short_name=\"GLDAS_NOAH025_M\",\n",
    "                    version=\"2.1\",\n",
    "                    temporal=(start_date, end_date),\n",
    "                    bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "                )\n",
    "                if granules:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise RuntimeError(f\"Failed to search for granules after {max_retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                continue\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No GLDAS granules found for the specified parameters\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        # Process data in temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files with progress tracking\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise RuntimeError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files with detailed error handling\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            failed_files = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print dimensions and variables for first file\n",
    "                    if processed_count == 0:\n",
    "                        print(f\"\\nFile structure: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Available variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Validate dataset structure\n",
    "                    required_dims = {'time', 'lat', 'lon'}\n",
    "                    if not all(dim in ds.dims for dim in required_dims):\n",
    "                        raise ValueError(f\"Missing required dimensions: {required_dims - set(ds.dims)}\")\n",
    "                    \n",
    "                    # Select soil moisture variables\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var]\n",
    "                    if not soil_vars:\n",
    "                        print(f\"Warning: No soil moisture variables found in {os.path.basename(file_path)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Apply spatial subsetting with validation\n",
    "                    ds = ds.sel(\n",
    "                        lat=slice(lat_bounds[0], lat_bounds[1]),\n",
    "                        lon=slice(lon_bounds[0], lon_bounds[1])\n",
    "                    )\n",
    "                    \n",
    "                    # Validate data content\n",
    "                    if ds.sizes['lat'] == 0 or ds.sizes['lon'] == 0:\n",
    "                        raise ValueError(\"No data points within specified bounds\")\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed_files.append((os.path.basename(file_path), str(e)))\n",
    "                    continue\n",
    "            \n",
    "            # Report processing results\n",
    "            if failed_files:\n",
    "                print(\"\\nWarning: Some files failed to process:\")\n",
    "                for fname, error in failed_files:\n",
    "                    print(f\"  - {fname}: {error}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets with error handling\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time')\n",
    "                combined_ds = combined_ds.sortby('time')  # Ensure temporal ordering\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to combine datasets: {str(e)}\")\n",
    "            \n",
    "            # Generate comprehensive dataset report\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Time resolution: {np.median(np.diff(combined_ds.time.values)).astype('timedelta64[h]')}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Spatial coverage: {combined_ds.sizes['lat']}x{combined_ds.sizes['lon']} grid points\")\n",
    "            print(f\"Lat range: {float(combined_ds.lat.min().values):.3f} to {float(combined_ds.lat.max().values):.3f}\")\n",
    "            print(f\"Lon range: {float(combined_ds.lon.min().values):.3f} to {float(combined_ds.lon.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate and report statistics for each layer\n",
    "            print(\"\\nSoil Moisture Statistics by Layer:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get variable metadata\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min (0th):   {percentiles[0]:.4f}\")\n",
    "                        print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                        print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                        print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                        print(f\"  Max (100th):  {percentiles[4]:.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                        \n",
    "                        # Check for potentially anomalous values\n",
    "                        q1, q3 = percentiles[1], percentiles[3]\n",
    "                        iqr = q3 - q1\n",
    "                        outliers = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))\n",
    "                        if outliers > 0:\n",
    "                            print(f\"  Potential outliers: {outliers:,} points\")\n",
    "                    else:\n",
    "                        print(\"Warning: No valid numeric data found\")\n",
    "            \n",
    "            # Export the dataset\n",
    "            export_filepath = export_dataset(combined_ds, start_date, end_date, output_dir)\n",
    "            print(f\"\\nData exported to: {export_filepath}\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nError retrieving GLDAS soil moisture data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLDAS data retrieval with progress tracking\n",
    "def validate_dates(start_date: str, end_date: str) -> tuple:\n",
    "    \"\"\"Validate and parse date strings.\"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        if end < start:\n",
    "            raise ValueError(\"End date must be after start date\")\n",
    "        return start, end\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Please use YYYY-MM-DD format. Error: {str(e)}\")\n",
    "\n",
    "def validate_bounds(lat_bounds: tuple, lon_bounds: tuple) -> None:\n",
    "    \"\"\"Validate spatial bounds for FLDAS data.\"\"\"\n",
    "    if not isinstance(lat_bounds, tuple) or not isinstance(lon_bounds, tuple):\n",
    "        raise TypeError(\"Bounds must be tuples\")\n",
    "    if len(lat_bounds) != 2 or len(lon_bounds) != 2:\n",
    "        raise ValueError(\"Bounds must contain exactly 2 values\")\n",
    "    if not (-60 <= lat_bounds[0] <= 90 and -60 <= lat_bounds[1] <= 90):\n",
    "        raise ValueError(\"FLDAS latitude must be between -60 and 90 degrees\")\n",
    "    if not (-180 <= lon_bounds[0] <= 180 and -180 <= lon_bounds[1] <= 180):\n",
    "        raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "    if lat_bounds[0] >= lat_bounds[1]:\n",
    "        raise ValueError(\"Minimum latitude must be less than maximum latitude\")\n",
    "    if lon_bounds[0] >= lon_bounds[1]:\n",
    "        raise ValueError(\"Minimum longitude must be less than maximum longitude\")\n",
    "\n",
    "def export_dataset(ds: xr.Dataset, start_date: str, end_date: str, output_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export dataset to NetCDF file with date-specific filename.\n",
    "    \"\"\"\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = '.'\n",
    "        \n",
    "    filename = f\"fldas_data_{start_date}_{end_date}.nc\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add export metadata\n",
    "    ds.attrs['export_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds.attrs['data_period'] = f\"{start_date} to {end_date}\"\n",
    "    ds.attrs['data_source'] = \"FLDAS_NOAH01_C_GL_M.001\"\n",
    "    \n",
    "    print(f\"\\nExporting data to {filepath}...\")\n",
    "    ds.to_netcdf(filepath)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def get_fldas_data(start_date: str, end_date: str, lat_bounds: tuple, lon_bounds: tuple, \n",
    "                   output_dir: str = None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieve FLDAS soil moisture data with progress tracking, validation, and export.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    lat_bounds : tuple\n",
    "        (min_lat, max_lat) for the region of interest\n",
    "    lon_bounds : tuple\n",
    "        (min_lon, max_lon) for the region of interest\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output file (default: current directory)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with FLDAS soil moisture data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_dates(start_date, end_date)\n",
    "        validate_bounds(lat_bounds, lon_bounds)\n",
    "        \n",
    "        print(\"\\nRetrieving FLDAS soil moisture data...\")\n",
    "        print(f\"Time range: {start_date} to {end_date}\")\n",
    "        print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "        \n",
    "        # Authenticate with NASA Earthdata\n",
    "        try:\n",
    "            auth = earthaccess.login()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to authenticate with NASA Earthdata: {str(e)}\")\n",
    "        \n",
    "        # Search for granules with retry mechanism\n",
    "        max_retries = 3\n",
    "        granules = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nSearching for FLDAS granules (attempt {attempt + 1}/{max_retries})...\")\n",
    "                granules = earthaccess.search_data(\n",
    "                    short_name=\"FLDAS_NOAH01_C_GL_M\",\n",
    "                    version=\"001\",\n",
    "                    temporal=(start_date, end_date),\n",
    "                    bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "                )\n",
    "                if granules:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise RuntimeError(f\"Failed to search for granules after {max_retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                continue\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No FLDAS granules found for the specified parameters\")\n",
    "        \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Download files\n",
    "            print(\"\\nDownloading granules...\")\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise RuntimeError(\"Failed to download any granules\")\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(downloaded_files)} files\")\n",
    "            \n",
    "            # Process files\n",
    "            print(\"\\nProcessing downloaded files...\")\n",
    "            datasets = []\n",
    "            failed_files = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing files\", unit=\"file\"):\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Print information for first file\n",
    "                    if processed_count == 0:\n",
    "                        print(f\"\\nFile structure: {os.path.basename(file_path)}\")\n",
    "                        print(\"Dimensions:\", list(ds.dims))\n",
    "                        print(\"Available variables:\", list(ds.data_vars))\n",
    "                    \n",
    "                    # Select soil moisture variables\n",
    "                    soil_vars = [var for var in ds.data_vars if 'SoilMoi' in var and 'cm_tavg' in var]\n",
    "                    if not soil_vars:\n",
    "                        raise ValueError(f\"No soil moisture variables found\")\n",
    "                    ds = ds[soil_vars]\n",
    "                    \n",
    "                    # Handle FLDAS specific coordinate system\n",
    "                    if 'X' in ds.dims and 'Y' in ds.dims:\n",
    "                        ds = ds.sel(\n",
    "                            X=slice(lon_bounds[0], lon_bounds[1]),\n",
    "                            Y=slice(lat_bounds[0], lat_bounds[1])\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Expected X/Y coordinates not found in dataset\")\n",
    "                    \n",
    "                    # Validate data content\n",
    "                    if ds.sizes['X'] == 0 or ds.sizes['Y'] == 0:\n",
    "                        raise ValueError(\"No data points within specified bounds\")\n",
    "                    \n",
    "                    datasets.append(ds)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed_files.append((os.path.basename(file_path), str(e)))\n",
    "                    continue\n",
    "            \n",
    "            # Report processing results\n",
    "            if failed_files:\n",
    "                print(\"\\nWarning: Some files failed to process:\")\n",
    "                for fname, error in failed_files:\n",
    "                    print(f\"  - {fname}: {error}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found in downloaded files\")\n",
    "            \n",
    "            # Combine datasets\n",
    "            print(\"\\nCombining datasets...\")\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time')\n",
    "                combined_ds = combined_ds.sortby('time')\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to combine datasets: {str(e)}\")\n",
    "            \n",
    "            # Dataset report\n",
    "            print(\"\\nDataset Information:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(f\"Time resolution: {np.median(np.diff(combined_ds.time.values)).astype('timedelta64[h]')}\")\n",
    "            print(f\"Number of timesteps: {len(combined_ds.time)}\")\n",
    "            print(f\"Spatial coverage: {combined_ds.sizes['Y']}x{combined_ds.sizes['X']} grid points\")\n",
    "            print(f\"Y range: {float(combined_ds.Y.min().values):.3f} to {float(combined_ds.Y.max().values):.3f}\")\n",
    "            print(f\"X range: {float(combined_ds.X.min().values):.3f} to {float(combined_ds.X.max().values):.3f}\")\n",
    "            \n",
    "            # Calculate statistics by layer\n",
    "            print(\"\\nSoil Moisture Statistics by Layer:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for var in combined_ds.data_vars:\n",
    "                print(f\"\\nVariable: {var}\")\n",
    "                data = combined_ds[var].values\n",
    "                \n",
    "                # Get metadata\n",
    "                attrs = combined_ds[var].attrs\n",
    "                units = attrs.get('units', 'kg/m^2')\n",
    "                long_name = attrs.get('long_name', var)\n",
    "                \n",
    "                print(f\"Description: {long_name}\")\n",
    "                print(f\"Units: {units}\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                \n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    valid_data = data[~np.isnan(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        percentiles = np.nanpercentile(data, [0, 25, 50, 75, 100])\n",
    "                        print(\"Statistics:\")\n",
    "                        print(f\"  Mean:     {np.nanmean(data):.4f}\")\n",
    "                        print(f\"  Std Dev:  {np.nanstd(data):.4f}\")\n",
    "                        print(f\"  Min (0th):   {percentiles[0]:.4f}\")\n",
    "                        print(f\"  25th:     {percentiles[1]:.4f}\")\n",
    "                        print(f\"  Median:   {percentiles[2]:.4f}\")\n",
    "                        print(f\"  75th:     {percentiles[3]:.4f}\")\n",
    "                        print(f\"  Max (100th):  {percentiles[4]:.4f}\")\n",
    "                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                        print(f\"  Data Coverage:   {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                        \n",
    "                        # Check for outliers\n",
    "                        q1, q3 = percentiles[1], percentiles[3]\n",
    "                        iqr = q3 - q1\n",
    "                        outliers = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))\n",
    "                        if outliers > 0:\n",
    "                            print(f\"  Potential outliers: {outliers:,} points\")\n",
    "                    else:\n",
    "                        print(\"Warning: No valid numeric data found\")\n",
    "            \n",
    "            # Export the dataset\n",
    "            export_filepath = export_dataset(combined_ds, start_date, end_date, output_dir)\n",
    "            print(f\"\\nData exported to: {export_filepath}\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nError retrieving FLDAS soil moisture data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAP data retrieval with progress tracking\n",
    "def get_smap_data(start_date, end_date, lat_bounds, lon_bounds, max_files=10):\n",
    "    \"\"\"\n",
    "    Retrieve SMAP L4 soil moisture data with missing value handling\n",
    "    \"\"\"\n",
    "    print(\"\\nRetrieving SMAP soil moisture data...\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")\n",
    "    print(f\"Spatial bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    \n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        \n",
    "        granules = earthaccess.search_data(\n",
    "            count=max_files,\n",
    "            short_name=\"SPL4SMGP\",\n",
    "            temporal=(start_date, end_date),\n",
    "            bounding_box=(lon_bounds[0], lat_bounds[0], lon_bounds[1], lat_bounds[1])\n",
    "        )\n",
    "        \n",
    "        if not granules:\n",
    "            raise ValueError(\"No SMAP granules found\")\n",
    "            \n",
    "        print(f\"Found {len(granules)} granules\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            downloaded_files = earthaccess.download(\n",
    "                granules,\n",
    "                local_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            if not downloaded_files:\n",
    "                raise ValueError(\"Failed to download granules\")\n",
    "                \n",
    "            datasets = []\n",
    "            for file_path in tqdm(downloaded_files, desc=\"Processing\"):\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as f:\n",
    "                        if len(datasets) == 0:\n",
    "                            print(\"\\nFile structure:\")\n",
    "                            def print_structure(name, obj):\n",
    "                                if isinstance(obj, h5py.Dataset):\n",
    "                                    print(f\"{name}:\")\n",
    "                                    print(f\"  Shape: {obj.shape}\")\n",
    "                                    print(f\"  Dtype: {obj.dtype}\")\n",
    "                                    if '_FillValue' in obj.attrs:\n",
    "                                        print(f\"  Fill Value: {obj.attrs['_FillValue']}\")\n",
    "                            f.visititems(print_structure)\n",
    "                        \n",
    "                        if 'Geophysical_Data' in f:\n",
    "                            geo_data = f['Geophysical_Data']\n",
    "                            sm_vars = ['sm_surface', 'sm_rootzone', 'sm_profile']\n",
    "                            ds_dict = {}\n",
    "                            \n",
    "                            time_value = None\n",
    "                            for attr in f.attrs.keys():\n",
    "                                if 'time' in attr.lower():\n",
    "                                    try:\n",
    "                                        time_str = f.attrs[attr]\n",
    "                                        if isinstance(time_str, bytes):\n",
    "                                            time_str = time_str.decode('utf-8')\n",
    "                                        time_value = pd.to_datetime(time_str)\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        continue\n",
    "                            \n",
    "                            if time_value is None:\n",
    "                                time_value = pd.Timestamp(start_date)\n",
    "                            \n",
    "                            for var in sm_vars:\n",
    "                                if var in geo_data:\n",
    "                                    # Get the data and attributes\n",
    "                                    data = geo_data[var][:]\n",
    "                                    attrs = dict(geo_data[var].attrs)\n",
    "                                    \n",
    "                                    # Handle missing values\n",
    "                                    # Check for _FillValue in attributes\n",
    "                                    fill_value = attrs.get('_FillValue', -9999)\n",
    "                                    # Replace both -9999 and the fill_value with NaN\n",
    "                                    data = np.where(data == -9999, np.nan, data)\n",
    "                                    if fill_value != -9999:\n",
    "                                        data = np.where(data == fill_value, np.nan, data)\n",
    "                                    \n",
    "                                    y_size, x_size = data.shape\n",
    "                                    coords = {\n",
    "                                        'y': np.linspace(lat_bounds[0], lat_bounds[1], y_size),\n",
    "                                        'x': np.linspace(lon_bounds[0], lon_bounds[1], x_size),\n",
    "                                        'time': [time_value]\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Print statistics for this variable\n",
    "                                    print(f\"\\nStatistics for {var}:\")\n",
    "                                    valid_data = data[~np.isnan(data)]\n",
    "                                    if len(valid_data) > 0:\n",
    "                                        print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                                        print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                                        print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                                        print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                                        print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                                        print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                                        print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "                                        print(f\"  Units:           {attrs.get('units', 'unknown')}\")\n",
    "                                    \n",
    "                                    da = xr.DataArray(\n",
    "                                        data[np.newaxis, :, :],\n",
    "                                        dims=['time', 'y', 'x'],\n",
    "                                        coords=coords,\n",
    "                                        name=var,\n",
    "                                        attrs=attrs\n",
    "                                    )\n",
    "                                    ds_dict[var] = da\n",
    "                            \n",
    "                            if ds_dict:\n",
    "                                ds = xr.Dataset(ds_dict)\n",
    "                                datasets.append(ds)\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process {os.path.basename(file_path)}: {str(e)}\")\n",
    "            \n",
    "            if not datasets:\n",
    "                raise ValueError(\"No valid soil moisture data found\")\n",
    "            \n",
    "            combined_ds = xr.concat(datasets, dim='time')\n",
    "            print(\"\\nRetrieved data summary:\")\n",
    "            print(f\"Time period: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}\")\n",
    "            print(\"Variables:\", list(combined_ds.data_vars))\n",
    "            \n",
    "            # Print final statistics for combined dataset\n",
    "            print(\"\\nFinal Dataset Statistics:\")\n",
    "            for var in combined_ds.data_vars:\n",
    "                data = combined_ds[var].values\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                print(f\"\\n{var}:\")\n",
    "                if len(valid_data) > 0:\n",
    "                    print(f\"  Mean:     {np.mean(valid_data):.4f}\")\n",
    "                    print(f\"  Std Dev:  {np.std(valid_data):.4f}\")\n",
    "                    print(f\"  Min:      {np.min(valid_data):.4f}\")\n",
    "                    print(f\"  Max:      {np.max(valid_data):.4f}\")\n",
    "                    print(f\"  Valid Points:    {len(valid_data):,}\")\n",
    "                    print(f\"  Missing Points:  {np.sum(np.isnan(data)):,}\")\n",
    "                    print(f\"  Coverage:        {(len(valid_data) / data.size * 100):.1f}%\")\n",
    "            \n",
    "            return combined_ds\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERRA-2 data retrieval with progress tracking\n",
    "\n",
    "def get_merra2_data(\n",
    "    bbox: Tuple[float, float, float, float],\n",
    "    date_range: Tuple[str, str],\n",
    "    var_names: Optional[List[str]] = None,\n",
    "    output_file: str = 'merra_2_soil_moisture_data.nc',\n",
    "    product: str = 'M2T1NXLND_5.12.4'\n",
    ") -> Optional[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Fetch MERRA-2 data and perform basic statistical analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bbox : tuple\n",
    "        Bounding box coordinates as (min_lon, min_lat, max_lon, max_lat)\n",
    "        Valid ranges: longitude [-180, 180], latitude [-90, 90]\n",
    "    \n",
    "    date_range : tuple\n",
    "        Start and end dates as ('YYYY-MM-DD', 'YYYY-MM-DD')\n",
    "    \n",
    "    var_names : list, optional\n",
    "        List of variable names to fetch. If None, defaults to \n",
    "        ['SFMC', 'GWETTOP', 'PRMC', 'RZMC']\n",
    "    \n",
    "    output_file : str, optional\n",
    "        Output filename for NetCDF data\n",
    "        Default: 'merra_2_soil_moisture_data.nc'\n",
    "    \n",
    "    product : str, optional\n",
    "        MERRA-2 product identifier\n",
    "        Default: 'M2T1NXLND_5.12.4'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset or None\n",
    "        Dataset containing the requested variables with statistics printed\n",
    "        Returns None if the request fails\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> # Fetch data for New York State\n",
    "    >>> ds = fetch_merra2_data(\n",
    "    ...     bbox=(-79.77, 40.5, -71.85, 45.02),\n",
    "    ...     date_range=('2020-01-01', '2020-01-31'),\n",
    "    ...     var_names=['SFMC', 'PRMC']\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    # Parameter validation\n",
    "    try:\n",
    "        # Validate and process bbox\n",
    "        minlon, minlat, maxlon, maxlat = bbox\n",
    "        if not (-180 <= minlon <= 180 and -180 <= maxlon <= 180):\n",
    "            raise ValueError(\"Longitude must be between -180 and 180 degrees\")\n",
    "        if not (-90 <= minlat <= 90 and -90 <= maxlat <= 90):\n",
    "            raise ValueError(\"Latitude must be between -90 and 90 degrees\")\n",
    "        if minlon >= maxlon or minlat >= maxlat:\n",
    "            raise ValueError(\"Min values must be less than max values\")\n",
    "\n",
    "        # Validate and process dates\n",
    "        start_date, end_date = date_range\n",
    "        try:\n",
    "            datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Dates must be in YYYY-MM-DD format\")\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"Start date must be before end date\")\n",
    "\n",
    "        # Set default variables if none provided\n",
    "        if var_names is None:\n",
    "            var_names = ['SFMC', 'GWETTOP', 'PRMC', 'RZMC']\n",
    "            print(f\"Using default variables: {var_names}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Parameter validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize urllib PoolManager and set base URL\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    url = 'https://disc.gsfc.nasa.gov/service/subset/jsonwsp'\n",
    "    \n",
    "    def get_http_data(request):\n",
    "        hdrs = {'Content-Type': 'application/json',\n",
    "                'Accept': 'application/json'}\n",
    "        data = json.dumps(request)       \n",
    "        r = http.request('POST', url, body=data, headers=hdrs)\n",
    "        response = json.loads(r.data)   \n",
    "        if response['type'] == 'jsonwsp/fault':\n",
    "            print('API Error: faulty %s request' % response['methodname'])\n",
    "            sys.exit(1)\n",
    "        return response\n",
    "    \n",
    "    # Construct the subset request\n",
    "    subset_request = {\n",
    "        'methodname': 'subset',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'version': '1.0',\n",
    "        'args': {\n",
    "            'role': 'subset',\n",
    "            'start': start_date,\n",
    "            'end': end_date,\n",
    "            'box': [minlon, minlat, maxlon, maxlat],\n",
    "            'crop': True,\n",
    "            'data': [{'datasetId': product,\n",
    "                      'variable': varName\n",
    "                     } for varName in var_names]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Submit request and get job ID\n",
    "    response = get_http_data(subset_request)\n",
    "    myJobId = response['result']['jobId']\n",
    "    print('Job ID:', myJobId)\n",
    "    print('Initial status:', response['result']['Status'])\n",
    "    \n",
    "    # Monitor job status\n",
    "    status_request = {\n",
    "        'methodname': 'GetStatus',\n",
    "        'version': '1.0',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'args': {'jobId': myJobId}\n",
    "    }\n",
    "    \n",
    "    while response['result']['Status'] in ['Accepted', 'Running']:\n",
    "        time.sleep(5)\n",
    "        response = get_http_data(status_request)\n",
    "        status = response['result']['Status']\n",
    "        percent = response['result']['PercentCompleted']\n",
    "        print(f'Job status: {status} ({percent}% complete)')\n",
    "    \n",
    "    def download_netcdf(url, output_file):\n",
    "        print(f\"\\nDownloading data to {output_file}...\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(output_file, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully saved data to {output_file}\")\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Get results and download data\n",
    "    if response['result']['Status'] == 'Succeeded':\n",
    "        print('Job Finished:', response['result']['message'])\n",
    "        \n",
    "        result = requests.get('https://disc.gsfc.nasa.gov/api/jobs/results/'+myJobId)\n",
    "        try:\n",
    "            result.raise_for_status()\n",
    "            urls = result.text.split('\\n')\n",
    "            \n",
    "            success = False\n",
    "            for url in urls:\n",
    "                if url.strip():\n",
    "                    print(\"\\nAttempting download from:\", url)\n",
    "                    success = download_netcdf(url, output_file)\n",
    "                    if success:\n",
    "                        break\n",
    "            \n",
    "            if success:\n",
    "                # Load and analyze the data\n",
    "                ds = xr.open_dataset(output_file)\n",
    "                \n",
    "                print(\"\\n=== Dataset Information ===\")\n",
    "                print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "                print(f\"\\nVariables: {list(ds.data_vars)}\")\n",
    "                print(f\"\\nTime range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "                print(f\"Spatial extent: {ds.lon.values.min():.2f}°E to {ds.lon.values.max():.2f}°E, \"\n",
    "                      f\"{ds.lat.values.min():.2f}°N to {ds.lat.values.max():.2f}°N\")\n",
    "                \n",
    "                for var in ds.data_vars:\n",
    "                    print(f\"\\n=== Statistics for {var} ===\")\n",
    "                    data = ds[var]\n",
    "                    print(f\"Shape: {data.shape}\")\n",
    "                    print(f\"Missing values: {data.isnull().sum().values}\")\n",
    "                    print(f\"Mean: {float(data.mean()):.4f}\")\n",
    "                    print(f\"Min: {float(data.min()):.4f}\")\n",
    "                    print(f\"Max: {float(data.max()):.4f}\")\n",
    "                    print(f\"Standard deviation: {float(data.std()):.4f}\")\n",
    "                \n",
    "                return ds\n",
    "            else:\n",
    "                print(\"Failed to download data. Please check your credentials and try again.\")\n",
    "                return None\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Error getting download URLs:', e)\n",
    "            return None\n",
    "    else:\n",
    "        print('Job Failed:', response['fault']['code'])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "max_lat= 45.02\n",
    "min_lat= 40.5\n",
    "max_lon= -71.85\n",
    "min_lon= -79.77\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "lat_bounds = (min_lat, max_lat)\n",
    "lon_bounds = (min_lon, max_lon)\n",
    "area=(max_lat, min_lon, min_lat, max_lon) # (max_lat, min_lon, min_lat, max_lon)\n",
    "bbox=(min_lon, min_lat, max_lon, max_lat) # (min_lon, min_lat, max_lon, max_lat)\n",
    "date_range=(start_date, end_date) #start_dat ,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving ERA5 data...\n",
      "Dataset: reanalysis-era5-land\n",
      "Time range: 2023-01\n",
      "Spatial bounds: (45.02, -79.77, 40.5, -71.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 14:42:15,533 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-01 14:42:15,534 WARNING [2024-09-26T00:00:00] Should you have not yet migrated from the old CDS system to the new CDS, please check our [informative page](https://confluence.ecmwf.int/x/uINmFw) for guidance.\n",
      "2024-11-01 14:42:15,536 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-01 14:42:15,537 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-01 14:42:15,540 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting request to ERA5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 14:42:16,137 INFO Request ID is 60147992-c040-415e-9070-32a650f93d3c\n",
      "2024-11-01 14:42:16,294 INFO status has been updated to accepted\n",
      "2024-11-01 14:42:19,258 INFO status has been updated to running\n",
      "2024-11-01 14:42:21,828 INFO status has been updated to successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete. File size: 0.28 MB\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Dimensions: {'valid_time': 48, 'latitude': 46, 'longitude': 80}\n",
      "\n",
      "Variables:\n",
      "\n",
      "Variable: swvl1\n",
      "Description: Volumetric soil water layer 1\n",
      "Units: m**3 m**-3\n",
      "Shape: (48, 46, 80)\n",
      "Statistics:\n",
      "  Mean:     0.3555\n",
      "  Median:   0.3956\n",
      "  Std Dev:  0.0937\n",
      "  Variance: 0.0088\n",
      "  Min:      0.0100\n",
      "  Max:      0.5200\n",
      "  Valid Points:    170,784\n",
      "  Missing Points:  5,856\n",
      "  Data Coverage:   96.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/0tkgm2g11c9202ygt6n5r_qh0000gn/T/ipykernel_26293/1923655684.py:48: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# ERA5_land request:\n",
    "request = {\n",
    "    \"variable\": \"volumetric_soil_water_layer_1\",\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"year\": \"2023\",\n",
    "    \"month\": \"01\",\n",
    "    \"day\": [\"01\", \"02\"],\n",
    "    \"time\": [f\"{hour:02d}:00\" for hour in range(24)],\n",
    "    \"area\": area,  # [north, west, south, east]\n",
    "    \"format\": \"netcdf\"\n",
    " }\n",
    "    \n",
    "    # Get the data\n",
    "try:\n",
    "     era5_data = get_era5_data(\n",
    "        dataset=\"reanalysis-era5-land\",\n",
    "        request=request,\n",
    "        output_file=\"era5_soil_moisture.nc\"\n",
    "    )  \n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve ERA5 data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving NLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for NLDAS granules...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 17833.15it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [03:02<00:00,  1.38s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 323015.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:03<00:00, 37.72file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Number of timesteps: 132\n",
      "Dimensions: {'time': 132, 'lat': 36, 'lon': 63}\n",
      "\n",
      "Soil Moisture Statistics:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilM_0_10cm\n",
      "Description: Soil moisture content (0-10cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     29.3776\n",
      "  Median:   28.3492\n",
      "  Std Dev:  7.0062\n",
      "  Min:      2.9674\n",
      "  Max:      47.5999\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_10_40cm\n",
      "Description: Soil moisture content (10-40cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     88.4562\n",
      "  Median:   85.0910\n",
      "  Std Dev:  20.4660\n",
      "  Min:      16.7697\n",
      "  Max:      142.7994\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_40_100cm\n",
      "Description: Soil moisture content (40-100cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     163.7207\n",
      "  Median:   164.3904\n",
      "  Std Dev:  29.5161\n",
      "  Min:      12.0045\n",
      "  Max:      282.3860\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_100_200cm\n",
      "Description: Soil moisture content (100-200cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     263.4715\n",
      "  Median:   269.7338\n",
      "  Std Dev:  46.3734\n",
      "  Min:      20.0000\n",
      "  Max:      456.5619\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_0_100cm\n",
      "Description: Soil moisture content (0-100cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     281.5546\n",
      "  Median:   283.5311\n",
      "  Std Dev:  48.8255\n",
      "  Min:      51.5873\n",
      "  Max:      448.6649\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Variable: SoilM_0_200cm\n",
      "Description: Soil moisture content (0-200cm)\n",
      "Units: kg m-2\n",
      "Shape: (132, 36, 63)\n",
      "Statistics:\n",
      "  Mean:     545.0261\n",
      "  Median:   556.9470\n",
      "  Std Dev:  83.3693\n",
      "  Min:      77.1631\n",
      "  Max:      840.6978\n",
      "  Valid Points:    267,960\n",
      "  Missing Points:  31,416\n",
      "  Data Coverage:   89.5%\n",
      "\n",
      "Saving data to nldas_data_2013-01-01_2023-12-31.nc...\n",
      "File saved successfully. Size: 4.60 MB\n"
     ]
    }
   ],
   "source": [
    "# Example usage NLDAS:\n",
    "if __name__ == \"__main__\":\n",
    "    start_date = start_date\n",
    "    end_date = end_date\n",
    "    lat_bounds = lat_bounds\n",
    "    lon_bounds = lon_bounds\n",
    "    \n",
    "    try:\n",
    "        # Set up output file name\n",
    "        output_file = f\"nldas_data_{start_date}_{end_date}.nc\"\n",
    "        \n",
    "        # Retrieve the data\n",
    "        nldas_data = get_nldas_data(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            lat_bounds=lat_bounds,\n",
    "            lon_bounds=lon_bounds\n",
    "        )\n",
    "        \n",
    "        # Save the data\n",
    "        print(f\"\\nSaving data to {output_file}...\")\n",
    "        nldas_data.to_netcdf(output_file)\n",
    "        \n",
    "        # Print file size\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"File saved successfully. Size: {file_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve NLDAS data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving GLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for GLDAS granules (attempt 1/3)...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 16019.45it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [06:58<00:00,  3.17s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 296068.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/132 [00:00<00:22,  5.75file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure: GLDAS_NOAH025_M.A201301.021.nc4\n",
      "Dimensions: ['time', 'bnds', 'lon', 'lat']\n",
      "Available variables: ['time_bnds', 'Swnet_tavg', 'Lwnet_tavg', 'Qle_tavg', 'Qh_tavg', 'Qg_tavg', 'Snowf_tavg', 'Rainf_tavg', 'Evap_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'AvgSurfT_inst', 'Albedo_inst', 'SWE_inst', 'SnowDepth_inst', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'SoilTMP100_200cm_inst', 'PotEvap_tavg', 'ECanop_tavg', 'Tveg_tavg', 'ESoil_tavg', 'RootMoist_inst', 'CanopInt_inst', 'Wind_f_inst', 'Rainf_f_tavg', 'Tair_f_inst', 'Qair_f_inst', 'Psurf_f_inst', 'SWdown_f_tavg', 'LWdown_f_tavg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:02<00:00, 53.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Time resolution: 744 hours\n",
      "Number of timesteps: 132\n",
      "Spatial coverage: 18x32 grid points\n",
      "Lat range: 40.625 to 44.875\n",
      "Lon range: -79.625 to -71.875\n",
      "\n",
      "Soil Moisture Statistics by Layer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilMoi0_10cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     31.4795\n",
      "  Std Dev:  6.4977\n",
      "  Min (0th):   8.3211\n",
      "  25th:     26.7874\n",
      "  Median:   30.1789\n",
      "  75th:     34.7768\n",
      "  Max (100th):  47.5826\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 1,444 points\n",
      "\n",
      "Variable: SoilMoi10_40cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     95.2638\n",
      "  Std Dev:  21.2855\n",
      "  Min (0th):   20.6895\n",
      "  25th:     80.3381\n",
      "  Median:   90.7238\n",
      "  75th:     107.9240\n",
      "  Max (100th):  142.7901\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 190 points\n",
      "\n",
      "Variable: SoilMoi40_100cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     174.2118\n",
      "  Std Dev:  35.5878\n",
      "  Min (0th):   30.2995\n",
      "  25th:     153.6755\n",
      "  Median:   172.8446\n",
      "  75th:     196.0735\n",
      "  Max (100th):  285.3114\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 2,103 points\n",
      "\n",
      "Variable: SoilMoi100_200cm_inst\n",
      "Description: Soil moisture\n",
      "Units: kg m-2\n",
      "Shape: (132, 18, 32)\n",
      "Statistics:\n",
      "  Mean:     287.1544\n",
      "  Std Dev:  44.7035\n",
      "  Min (0th):   56.4324\n",
      "  25th:     260.5154\n",
      "  Median:   287.0990\n",
      "  75th:     318.6728\n",
      "  Max (100th):  450.7496\n",
      "  Valid Points:    68,244\n",
      "  Missing Points:  7,788\n",
      "  Data Coverage:   89.8%\n",
      "  Potential outliers: 1,416 points\n",
      "\n",
      "Exporting data to ./gldas_data_2013-01-01_2023-12-31.nc...\n",
      "Export complete!\n",
      "\n",
      "Data exported to: ./gldas_data_2013-01-01_2023-12-31.nc\n"
     ]
    }
   ],
   "source": [
    "# GLDAS example\n",
    "gldas_data = get_gldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving FLDAS soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "\n",
      "Searching for FLDAS granules (attempt 1/3)...\n",
      "Found 132 granules\n",
      "\n",
      "Downloading granules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 132/132 [00:00<00:00, 13187.75it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 132/132 [33:46<00:00, 15.35s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 132/132 [00:00<00:00, 349084.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 132 files\n",
      "\n",
      "Processing downloaded files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/132 [00:00<00:22,  5.83file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure: FLDAS_NOAH01_C_GL_M.A201301.001.nc\n",
      "Dimensions: ['time', 'bnds', 'X', 'Y']\n",
      "Available variables: ['time_bnds', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'Psurf_f_tavg', 'Qair_f_tavg', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_tavg', 'Qsb_tavg', 'RadT_tavg', 'Rainf_f_tavg', 'SWE_inst', 'SWdown_f_tavg', 'SnowCover_inst', 'SnowDepth_inst', 'Snowf_tavg', 'Swnet_tavg', 'Tair_f_tavg', 'Wind_f_tavg', 'SoilMoi00_10cm_tavg', 'SoilMoi10_40cm_tavg', 'SoilMoi40_100cm_tavg', 'SoilMoi100_200cm_tavg', 'SoilTemp00_10cm_tavg', 'SoilTemp10_40cm_tavg', 'SoilTemp40_100cm_tavg', 'SoilTemp100_200cm_tavg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 132/132 [00:01<00:00, 70.00file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining datasets...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Time range: 2013-01-01T00:00:00.000000000 to 2023-12-01T00:00:00.000000000\n",
      "Time resolution: 744 hours\n",
      "Number of timesteps: 132\n",
      "Spatial coverage: 45x79 grid points\n",
      "Y range: 40.550 to 44.950\n",
      "X range: -79.750 to -71.950\n",
      "\n",
      "Soil Moisture Statistics by Layer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Variable: SoilMoi00_10cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3622\n",
      "  Std Dev:  0.0497\n",
      "  Min (0th):   0.1411\n",
      "  25th:     0.3361\n",
      "  Median:   0.3690\n",
      "  75th:     0.3959\n",
      "  Max (100th):  0.4678\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,172 points\n",
      "\n",
      "Variable: SoilMoi10_40cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3800\n",
      "  Std Dev:  0.0672\n",
      "  Min (0th):   0.0897\n",
      "  25th:     0.3452\n",
      "  Median:   0.3867\n",
      "  75th:     0.4292\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 13,134 points\n",
      "\n",
      "Variable: SoilMoi40_100cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3456\n",
      "  Std Dev:  0.0804\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3084\n",
      "  Median:   0.3695\n",
      "  75th:     0.4006\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 23,405 points\n",
      "\n",
      "Variable: SoilMoi100_200cm_tavg\n",
      "Description: soil moisture content\n",
      "Units: m^3 m-3\n",
      "Shape: (132, 45, 79)\n",
      "Statistics:\n",
      "  Mean:     0.3592\n",
      "  Std Dev:  0.0597\n",
      "  Min (0th):   0.0800\n",
      "  25th:     0.3327\n",
      "  Median:   0.3670\n",
      "  75th:     0.3973\n",
      "  Max (100th):  0.4680\n",
      "  Valid Points:    420,156\n",
      "  Missing Points:  49,104\n",
      "  Data Coverage:   89.5%\n",
      "  Potential outliers: 20,378 points\n",
      "\n",
      "Exporting data to ./fldas_data_2013-01-01_2023-12-31.nc...\n",
      "Export complete!\n",
      "\n",
      "Data exported to: ./fldas_data_2013-01-01_2023-12-31.nc\n"
     ]
    }
   ],
   "source": [
    "# FLDAS example\n",
    "fldas_data = get_fldas_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving SMAP soil moisture data...\n",
      "Time range: 2013-01-01 to 2023-12-31\n",
      "Spatial bounds: lat (40.5, 45.02), lon (-79.77, -71.85)\n",
      "Found 10 granules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 10/10 [00:00<00:00, 1405.27it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 10/10 [03:30<00:00, 21.01s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 10/10 [00:00<00:00, 91779.08it/s]\n",
      "Processing:  10%|█         | 1/10 [00:00<00:01,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File structure:\n",
      "EASE2_global_projection:\n",
      "  Shape: (1,)\n",
      "  Dtype: |S1\n",
      "Geophysical_Data/baseflow_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/depth_to_water_table_from_surface_in_peat:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/free_surface_water_on_peat_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_ground:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_latent:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/heat_flux_sensible:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/height_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_evapotranspiration_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_saturated:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_snow_covered:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_unsaturated:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/land_fraction_wilting:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/leaf_area_index:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/mwrtm_vegopacity:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/net_downward_longwave_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/net_downward_shortwave_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/overland_runoff_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/precipitation_total_surface_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/radiation_longwave_absorbed_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/radiation_shortwave_downward_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile_pctl:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_profile_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone_pctl:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_rootzone_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_surface:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/sm_surface_wetness:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_depth:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_mass:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snow_melt_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/snowfall_surface_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer1:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer2:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer3:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer4:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer5:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_temp_layer6:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/soil_water_infiltration_flux:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/specific_humidity_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/surface_pressure:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/surface_temp:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/temp_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/vegetation_greenness_fraction:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "Geophysical_Data/windspeed_lowatmmodlay:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_column:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: uint32\n",
      "  Fill Value: 4294967294\n",
      "cell_lat:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_lon:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: float32\n",
      "  Fill Value: -9999.0\n",
      "cell_row:\n",
      "  Shape: (1624, 3856)\n",
      "  Dtype: uint32\n",
      "  Fill Value: 4294967294\n",
      "time:\n",
      "  Shape: (1,)\n",
      "  Dtype: float64\n",
      "x:\n",
      "  Shape: (3856,)\n",
      "  Dtype: float64\n",
      "y:\n",
      "  Shape: (1624,)\n",
      "  Dtype: float64\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2136\n",
      "  Std Dev:  0.1390\n",
      "  Min:      0.0080\n",
      "  Max:      0.8854\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0063\n",
      "  Max:      0.9311\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 2/10 [00:00<00:01,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2135\n",
      "  Std Dev:  0.1389\n",
      "  Min:      0.0043\n",
      "  Max:      0.8869\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0064\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2135\n",
      "  Std Dev:  0.1389\n",
      "  Min:      0.0039\n",
      "  Max:      0.8894\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 3/10 [00:00<00:01,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std Dev:  0.1674\n",
      "  Min:      0.0064\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9295\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2131\n",
      "  Std Dev:  0.1391\n",
      "  Min:      0.0027\n",
      "  Max:      0.9020\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0065\n",
      "  Max:      0.9308\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 5/10 [00:00<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2125\n",
      "  Std Dev:  0.1393\n",
      "  Min:      0.0035\n",
      "  Max:      0.8945\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1675\n",
      "  Min:      0.0063\n",
      "  Max:      0.9305\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 6/10 [00:00<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std Dev:  0.1392\n",
      "  Min:      0.0067\n",
      "  Max:      0.8925\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2414\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9302\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2122\n",
      "  Std Dev:  0.1393\n",
      "  Min:      0.0093\n",
      "  Max:      0.8780\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2414\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9300\n",
      "  Valid Points:    1,684,725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 8/10 [00:01<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2559\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9294\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2125\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0094\n",
      "  Max:      0.8894\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0064\n",
      "  Max:      0.9309\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 9/10 [00:01<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2129\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0088\n",
      "  Max:      0.8882\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0065\n",
      "  Max:      0.9308\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_surface:\n",
      "  Mean:     0.2130\n",
      "  Std Dev:  0.1394\n",
      "  Min:      0.0062\n",
      "  Max:      0.8893\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:01<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for sm_rootzone:\n",
      "  Mean:     0.2416\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0066\n",
      "  Max:      0.9306\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Statistics for sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0077\n",
      "  Max:      0.9296\n",
      "  Valid Points:    1,684,725\n",
      "  Missing Points:  4,577,419\n",
      "  Coverage:        26.9%\n",
      "  Units:           b'm3 m-3'\n",
      "\n",
      "Retrieved data summary:\n",
      "Time period: 2013-01-01T00:00:00.000000000 to 2013-01-01T00:00:00.000000000\n",
      "Variables: ['sm_surface', 'sm_rootzone', 'sm_profile']\n",
      "\n",
      "Final Dataset Statistics:\n",
      "\n",
      "sm_surface:\n",
      "  Mean:     0.2129\n",
      "  Std Dev:  0.1392\n",
      "  Min:      0.0027\n",
      "  Max:      0.9020\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n",
      "\n",
      "sm_rootzone:\n",
      "  Mean:     0.2415\n",
      "  Std Dev:  0.1674\n",
      "  Min:      0.0063\n",
      "  Max:      0.9311\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n",
      "\n",
      "sm_profile:\n",
      "  Mean:     0.2560\n",
      "  Std Dev:  0.1740\n",
      "  Min:      0.0076\n",
      "  Max:      0.9296\n",
      "  Valid Points:    16,847,250\n",
      "  Missing Points:  45,774,190\n",
      "  Coverage:        26.9%\n"
     ]
    }
   ],
   "source": [
    "# SMAP example\n",
    "smap_data = get_smap_data(start_date, end_date, lat_bounds, lon_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 67239b288be99c3db6e2b0ad\n",
      "Initial status: Accepted\n",
      "Job status: Succeeded (100% complete)\n",
      "Job Finished: Complete (M2T1NXLND_5.12.4)\n",
      "\n",
      "Attempting download from: https://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2T1NXLND.5.12.4/doc/MERRA2.README.pdf\n",
      "\n",
      "Downloading data to merra_2_soil_moisture_data.nc...\n",
      "Error downloading data: 404 Client Error: Not Found for url: https://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2T1NXLND.5.12.4/doc/MERRA2.README.pdf%0D\n",
      "\n",
      "Attempting download from: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXLND.5.12.4/2020/01/MERRA2_400.tavg1_2d_lnd_Nx.20200101.nc4.nc4?SFMC[0:23][261:270][160:173],PRMC[0:23][261:270][160:173],time,lat[261:270],lon[160:173]\n",
      "\n",
      "Downloading data to merra_2_soil_moisture_data.nc...\n",
      "Successfully saved data to merra_2_soil_moisture_data.nc\n",
      "\n",
      "=== Dataset Information ===\n",
      "Dimensions: {'time': 24, 'lat': 10, 'lon': 14}\n",
      "\n",
      "Variables: ['SFMC', 'PRMC']\n",
      "\n",
      "Time range: 2020-01-01T00:30:00.000000000 to 2020-01-01T23:30:00.000000000\n",
      "Spatial extent: -80.00°E to -71.88°E, 40.50°N to 45.00°N\n",
      "\n",
      "=== Statistics for SFMC ===\n",
      "Shape: (24, 10, 14)\n",
      "Missing values: 0\n",
      "Mean: 0.2950\n",
      "Min: 0.1709\n",
      "Max: 0.3823\n",
      "Standard deviation: 0.0403\n",
      "\n",
      "=== Statistics for PRMC ===\n",
      "Shape: (24, 10, 14)\n",
      "Missing values: 0\n",
      "Mean: 0.2829\n",
      "Min: 0.1565\n",
      "Max: 0.3816\n",
      "Standard deviation: 0.0463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/0tkgm2g11c9202ygt6n5r_qh0000gn/T/ipykernel_94552/3139272757.py:169: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# MERRA-2 data \n",
    "merra2_data = get_merra2_data(\n",
    "    bbox=bbox,  # (min_lon, min_lat, max_lon, max_lat)\n",
    "    date_range= date_range,\n",
    "    var_names=['SFMC', 'PRMC'] \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
